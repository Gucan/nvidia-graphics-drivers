From bffd7eeb85a98254280ddd4510f4a0dc7f861156 Mon Sep 17 00:00:00 2001
From: Alberto Milone <alberto.milone@canonical.com>
Date: Thu, 9 Jul 2020 16:42:32 +0200
Subject: [PATCH 1/1] Add support for Linux 5.8

---
 common/inc/nv-linux.h                       |   4 +
 common/inc/nv-mm.h                          |  54 +++++++
 common/inc/nv-procfs.h                      |  54 ++++++-
 conftest.sh                                 |  88 ++++++++++
 nvidia-drm/nvidia-drm-linux.c               |   4 +-
 nvidia-drm/nvidia-drm.Kbuild                |   1 +
 nvidia-uvm/uvm8.c                           |  48 +++---
 nvidia-uvm/uvm8_ats_faults.h                |   2 +-
 nvidia-uvm/uvm8_gpu_access_counters.c       |   4 +-
 nvidia-uvm/uvm8_gpu_non_replayable_faults.c |   4 +-
 nvidia-uvm/uvm8_gpu_replayable_faults.c     |   6 +-
 nvidia-uvm/uvm8_lock.c                      |   6 +-
 nvidia-uvm/uvm8_lock.h                      | 171 ++++++++++++--------
 nvidia-uvm/uvm8_lock_test.c                 |   4 +-
 nvidia-uvm/uvm8_mem.c                       |   4 +-
 nvidia-uvm/uvm8_migrate.c                   |  10 +-
 nvidia-uvm/uvm8_policy.c                    |  19 +--
 nvidia-uvm/uvm8_populate_pageable.c         |  12 +-
 nvidia-uvm/uvm8_tools.c                     |   4 +-
 nvidia-uvm/uvm8_va_block.c                  |  14 +-
 nvidia-uvm/uvm8_va_range.c                  |   4 +-
 nvidia-uvm/uvm8_va_range.h                  |   2 +-
 nvidia-uvm/uvm8_va_space.c                  |  24 +--
 nvidia-uvm/uvm8_va_space_mm.c               |  10 +-
 nvidia/linux_nvswitch.c                     |   1 +
 nvidia/nvidia.Kbuild                        |   2 +
 nvidia/nvlink_linux.c                       |   1 -
 nvidia/os-mlock.c                           |   8 +-
 28 files changed, 400 insertions(+), 165 deletions(-)

diff --git a/common/inc/nv-linux.h b/common/inc/nv-linux.h
index 0565dac..727f781 100644
--- a/common/inc/nv-linux.h
+++ b/common/inc/nv-linux.h
@@ -553,7 +553,11 @@ extern int nv_pat_mode;
 
 static inline void *nv_vmalloc(unsigned long size)
 {
+#if defined(NV_VMALLOC_HAS_PGPROT_T_ARG)
     void *ptr = __vmalloc(size, GFP_KERNEL, PAGE_KERNEL);
+#else
+    void *ptr = __vmalloc(size, GFP_KERNEL);
+#endif
     if (ptr)
         NV_MEMDBG_ADD(ptr, size);
     return ptr;
diff --git a/common/inc/nv-mm.h b/common/inc/nv-mm.h
index a801d4b..27f7f55 100644
--- a/common/inc/nv-mm.h
+++ b/common/inc/nv-mm.h
@@ -213,4 +213,58 @@ typedef int vm_fault_t;
     }
 #endif // NV_VM_FAULT_PRESENT
 
+static inline void nv_mmap_read_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_read_lock(mm);
+#else
+    down_read(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_read_unlock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_read_unlock(mm);
+#else
+    up_read(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_write_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_write_lock(mm);
+#else
+    down_write(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_write_unlock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_write_unlock(mm);
+#else
+    up_write(&mm->mmap_sem);
+#endif
+}
+
+static inline int nv_mm_rwsem_is_locked(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    return rwsem_is_locked(&mm->mmap_lock);
+#else
+    return rwsem_is_locked(&mm->mmap_sem);
+#endif
+}
+
+static inline struct rw_semaphore *nv_mmap_get_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    return &mm->mmap_lock;
+#else
+    return &mm->mmap_sem;
+#endif
+}
+
 #endif // __NV_MM_H__
diff --git a/common/inc/nv-procfs.h b/common/inc/nv-procfs.h
index e76325c..f754254 100644
--- a/common/inc/nv-procfs.h
+++ b/common/inc/nv-procfs.h
@@ -74,6 +74,18 @@ typedef struct file_operations nv_proc_ops_t;
     })
 #endif
 
+#if defined(NV_PROCFS_PROC_OPS_PRESENT)
+#define NV_CREATE_PROC_FILE(filename,parent,__name,__data)               \
+   ({                                                                    \
+        struct proc_dir_entry *__entry;                                  \
+        int mode = (S_IFREG | S_IRUGO);                                  \
+        const struct proc_ops *fops = &nv_procfs_##__name##_fops;		 \
+        if (fops->proc_write != 0)                                            \
+            mode |= S_IWUSR;                                             \
+        __entry = proc_create_data(filename, mode, parent, fops, __data);\
+        __entry;                                                         \
+    })
+#else
 #define NV_CREATE_PROC_FILE(filename,parent,__name,__data)               \
    ({                                                                    \
         struct proc_dir_entry *__entry;                                  \
@@ -85,6 +97,7 @@ typedef struct file_operations nv_proc_ops_t;
             __data);                                                     \
         __entry;                                                         \
     })
+#endif
 
 /*
  * proc_mkdir_mode exists in Linux 2.6.9, but isn't exported until Linux 3.0.
@@ -126,6 +139,45 @@ typedef struct file_operations nv_proc_ops_t;
     remove_proc_entry(entry->name, entry->parent);
 #endif
 
+#if defined(NV_PROCFS_PROC_OPS_PRESENT)
+#define NV_DEFINE_PROCFS_SINGLE_FILE(name, open_callback, close_callback)     \
+    static int nv_procfs_open_##name(                                         \
+        struct inode *inode,                                                  \
+        struct file *filep                                                    \
+    )                                                                         \
+    {                                                                         \
+        int ret;                                                              \
+        ret = single_open(filep, nv_procfs_read_##name,                       \
+                          NV_PDE_DATA(inode));                                \
+        if (ret < 0)                                                          \
+        {                                                                     \
+            return ret;                                                       \
+        }                                                                     \
+        ret = open_callback();                                                \
+        if (ret < 0)                                                          \
+        {                                                                     \
+            single_release(inode, filep);                                     \
+        }                                                                     \
+        return ret;                                                           \
+    }                                                                         \
+                                                                              \
+    static int nv_procfs_release_##name(                                      \
+        struct inode *inode,                                                  \
+        struct file *filep                                                    \
+    )                                                                         \
+    {                                                                         \
+        close_callback();                                                     \
+        return single_release(inode, filep);                                  \
+    }                                                                         \
+                                                                              \
+    static const struct proc_ops nv_procfs_##name##_fops = {                  \
+        .proc_open       = nv_procfs_open_##name,                             \
+        .proc_read       = seq_read,                                          \
+        .proc_lseek     = seq_lseek,                                          \
+        .proc_release    = nv_procfs_release_##name,                          \
+    };
+
+#else
 #define NV_DEFINE_PROCFS_SINGLE_FILE(__name)                                  \
     static int nv_procfs_open_##__name(                                       \
         struct inode *inode,                                                  \
@@ -143,7 +195,7 @@ typedef struct file_operations nv_proc_ops_t;
         .NV_PROC_OPS_LSEEK   = seq_lseek,                                     \
         .NV_PROC_OPS_RELEASE = single_release,                                \
     };
-
+#endif
 #endif  /* CONFIG_PROC_FS */
 
 #endif /* _NV_PROCFS_H */
diff --git a/conftest.sh b/conftest.sh
index 301463a..5973d5a 100755
--- a/conftest.sh
+++ b/conftest.sh
@@ -1458,6 +1458,40 @@ compile_test() {
             set_configuration "NV_TASK_STRUCT_HAS_CRED" "types"
         ;;
 
+        proc_create)
+            #
+            # Determine if the proc_*() function rely on file_operations.
+            #
+            # Added by commit a8ca16ea7b0a ("proc: Supply a function to
+            # Replaced by commit 97a32539b956 ("proc: convert everything
+            # to "struct proc_ops"
+            #
+            CODE="
+            #include <linux/proc_fs.h>
+            #include <linux/seq_file.h>
+
+            static int conftest_proc_show(struct seq_file *m, void *v) {
+                return 0;
+            }
+
+            static int conftest_proc_open(struct inode *inode, struct  file *file) {
+              return single_open(file, conftest_proc_show, NULL);
+            }
+
+            static const struct proc_ops conftest_proc_ops = {
+                .proc_open = conftest_proc_open,
+                .proc_read = seq_read,
+                .proc_lseek = seq_lseek,
+                .proc_release = single_release,
+            };
+
+            void conftest_proc_create(void) {
+                proc_create(\"conftest_proc\", 0, NULL, &conftest_proc_ops);
+            }"
+
+            compile_check_conftest "$CODE" "NV_PROCFS_PROC_OPS_PRESENT" "" "types"
+        ;;
+
         backing_dev_info)
             #
             # Determine if the 'address_space' structure has
@@ -3143,6 +3177,22 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_TIMER_SETUP_PRESENT" "" "functions"
         ;;
 
+        timeval_structs)
+            #
+            # Determine if timespec and timeval structs are present.
+            #
+            # Hidden by commit c766d1472c70 ("y2038: hide timeval/timespec/itimerval/
+            # itimerspec types") (2020-02-20)
+            #
+            CODE="
+            #include <linux/time.h>
+            void conftest_timeval_structs(void) {
+                struct timeval tmp;
+            }"
+            compile_check_conftest "$CODE" "NV_TIME_STRUCTS_PRESENT" "" "types"
+        ;;
+
+
         radix_tree_replace_slot)
             #
             # Determine if the radix_tree_replace_slot() function is
@@ -3683,6 +3733,44 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_KTIME_GET_REAL_TS64_PRESENT" "" "functions"
         ;;
 
+        vmalloc_has_pgprot_t_arg)
+            #
+            # Determine if __vmalloc has the 'pgprot' argument.
+            #
+            # The third argument to __vmalloc, page protection
+            # 'pgprot_t prot', was removed by commit 88dca4ca5a93
+            # (mm: remove the pgprot argument to __vmalloc)
+            # in v5.8-rc1 (2020-06-01).
+        CODE="
+        #include <linux/vmalloc.h>
+
+        void conftest_vmalloc_has_pgprot_t_arg(void) {
+            pgprot_t prot;
+            (void)__vmalloc(0, 0, prot);
+        }"
+
+            compile_check_conftest "$CODE" "NV_VMALLOC_HAS_PGPROT_T_ARG" "" "types"
+
+        ;;
+
+        mm_has_mmap_lock)
+            #
+            # Determine if the 'mm_struct' structure has a 'mmap_lock' field.
+            #
+            # Kernel commit da1c55f1b272 ("mmap locking API: rename mmap_sem
+            # to mmap_lock") replaced the field 'mmap_sem' by 'mmap_lock'
+            # in v5.8-rc1 (2020-06-08).
+            CODE="
+            #include <linux/mm_types.h>
+
+            int conftest_mm_has_mmap_lock(void) {
+                return offsetof(struct mm_struct, mmap_lock);
+            }"
+
+            compile_check_conftest "$CODE" "NV_MM_HAS_MMAP_LOCK" "" "types"
+
+        ;;
+
     esac
 }
 
diff --git a/nvidia-drm/nvidia-drm-linux.c b/nvidia-drm/nvidia-drm-linux.c
index 1d3e658..f8fcde4 100644
--- a/nvidia-drm/nvidia-drm-linux.c
+++ b/nvidia-drm/nvidia-drm-linux.c
@@ -103,11 +103,11 @@ int nv_drm_lock_user_pages(unsigned long address,
         return -ENOMEM;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
 
     pages_pinned = NV_GET_USER_PAGES(address, pages_count, write, force,
                                      user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
 
     if (pages_pinned < 0 || (unsigned)pages_pinned < pages_count) {
         goto failed;
diff --git a/nvidia-drm/nvidia-drm.Kbuild b/nvidia-drm/nvidia-drm.Kbuild
index c47ab77..a5af634 100644
--- a/nvidia-drm/nvidia-drm.Kbuild
+++ b/nvidia-drm/nvidia-drm.Kbuild
@@ -92,3 +92,4 @@ NV_CONFTEST_TYPE_COMPILE_TESTS += drm_atomic_helper_swap_state_has_stall_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_driver_prime_flag_present
 NV_CONFTEST_TYPE_COMPILE_TESTS += vm_fault_t
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_gem_object_has_resv
+NV_CONFTEST_TYPE_COMPILE_TESTS += mm_has_mmap_lock
diff --git a/nvidia-uvm/uvm8.c b/nvidia-uvm/uvm8.c
index fe024d5..9a6ab32 100644
--- a/nvidia-uvm/uvm8.c
+++ b/nvidia-uvm/uvm8.c
@@ -297,8 +297,9 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
         return;
     }
 
-    // At this point we are guaranteed that the mmap_sem is held in write mode.
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    // At this point we are guaranteed that the mmap_lock is held in write
+    // mode.
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     // Split vmas should always fall entirely within the old one, and be on one
     // side.
@@ -347,7 +348,7 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_close_managed(struct vm_area_struct *vma)
@@ -357,7 +358,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     bool make_zombie = false;
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(current->mm);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) == NV_OK);
 
@@ -385,7 +386,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
         }
     }
 
-    // See uvm_mmap for why we need this in addition to mmap_sem
+    // See uvm_mmap for why we need this in addition to mmap_lock
     uvm_va_space_down_write(va_space);
 
     uvm_destroy_vma_managed(vma, make_zombie);
@@ -401,7 +402,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     uvm_va_space_up_write(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
@@ -426,10 +427,10 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 
     service_context->cpu_fault.wakeup_time_stamp = 0;
 
-    // The mmap_sem might be held in write mode, but the mode doesn't matter for
-    // the purpose of lock ordering and we don't rely on it being in write
+    // The mmap_lock might be held in write mode, but the mode doesn't matter
+    // for the purpose of lock ordering and we don't rely on it being in write
     // anywhere so just record it as read mode in all cases.
-    uvm_record_lock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_lock_mmap_lock_read(vma->vm_mm);
 
     do {
         bool do_sleep = false;
@@ -489,7 +490,7 @@ out:
         uvm_gpu_retain_mask(&service_context->cpu_fault.gpus_to_check_for_ecc);
 
     uvm_va_space_up_read(va_space);
-    uvm_record_unlock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_read(vma->vm_mm);
 
     if (status == NV_OK) {
         uvm_gpu_t *gpu;
@@ -554,7 +555,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
     bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
     NV_STATUS status;
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -592,7 +593,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
 
     uvm_va_space_up_write(va_space);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 // vm operations on semaphore pool allocations only control CPU mappings. Unmapping GPUs,
@@ -602,7 +603,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(current->mm);
 
     uvm_va_space_down_read(va_space);
 
@@ -611,7 +612,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_up_read(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static struct vm_operations_struct uvm_vm_ops_semaphore_pool =
@@ -641,14 +642,15 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
     if (status != NV_OK)
         return -EBADFD;
 
-    // TODO: Bug 2667195: Fail with EOPNOTSUPP if the VA space is associated
-    //       with an mm, but current->mm doesn't match that mm. This is work
-    //       associated with bug 2667197, but it doesn't make sense to handle
-    //       this until the mm association is moved to VA space init. Until that
-    //       happens, the mm association could happen at UvmRegisterGpuVaSpace,
-    //       which may be too late to do this check.
+    // When the VA space is associated with an mm, all vmas under the VA space
+    // must come from that mm.
+    if (uvm_va_space_mm_enabled(va_space)) {
+        UVM_ASSERT(va_space->va_space_mm.mm);
+        if (va_space->va_space_mm.mm != current->mm)
+            return -EOPNOTSUPP;
+    }
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     // UVM mappings are required to set offset == VA. This simplifies things
     // since we don't have to worry about address aliasing (except for fork,
@@ -693,7 +695,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
     }
     vma_wrapper_allocated = true;
 
-    // The kernel has taken mmap_sem in write mode, but that doesn't prevent
+    // The kernel has taken mmap_lock in write mode, but that doesn't prevent
     // this va_space from being modified by the GPU fault path or from the ioctl
     // path where we don't have this mm for sure, so we have to lock the VA
     // space directly.
@@ -734,7 +736,7 @@ out:
     if (ret != 0 && vma_wrapper_allocated)
         uvm_vma_wrapper_destroy(vma->vm_private_data);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 
     return ret;
 }
diff --git a/nvidia-uvm/uvm8_ats_faults.h b/nvidia-uvm/uvm8_ats_faults.h
index e001b60..1d10534 100644
--- a/nvidia-uvm/uvm8_ats_faults.h
+++ b/nvidia-uvm/uvm8_ats_faults.h
@@ -39,7 +39,7 @@ NV_STATUS uvm_ats_invalidate_tlbs(uvm_gpu_va_space_t *gpu_va_space,
 static bool uvm_can_ats_service_faults(uvm_gpu_va_space_t *gpu_va_space, struct mm_struct *mm)
 {
     if (mm)
-        uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+        uvm_assert_mmap_lock_locked(mm);
     if (gpu_va_space->ats.enabled)
         UVM_ASSERT(g_uvm_global.ats.enabled);
 
diff --git a/nvidia-uvm/uvm8_gpu_access_counters.c b/nvidia-uvm/uvm8_gpu_access_counters.c
index faf2f40..77a13e4 100644
--- a/nvidia-uvm/uvm8_gpu_access_counters.c
+++ b/nvidia-uvm/uvm8_gpu_access_counters.c
@@ -1180,7 +1180,7 @@ static NV_STATUS service_phys_single_va_block(uvm_gpu_t *gpu,
         // in order to lock it before locking the VA space.
         mm = uvm_va_space_mm_retain(va_space);
         if (mm)
-            uvm_down_read_mmap_sem(&mm->mmap_sem);
+            uvm_down_read_mmap_lock(mm);
 
         // Re-check that the VA block is valid after taking the VA space lock
         uvm_va_space_down_read(va_space);
@@ -1222,7 +1222,7 @@ done:
         uvm_va_space_up_read(va_space);
 
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(mm);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
index 979ab35..81a82b0 100644
--- a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
@@ -505,7 +505,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
     // TODO: Bug 1867098: Taking mmap_sem here may deadlock between RM
     // and UVM.
     if (mm)
-        uvm_down_read_mmap_sem(&mm->mmap_sem);
+        uvm_down_read_mmap_lock(mm);
 
     uvm_va_space_down_read(va_space);
 
@@ -556,7 +556,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
 exit_no_channel:
     uvm_va_space_up_read(va_space);
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(mm);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_replayable_faults.c b/nvidia-uvm/uvm8_gpu_replayable_faults.c
index 7426584..0757d83 100644
--- a/nvidia-uvm/uvm8_gpu_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_replayable_faults.c
@@ -1430,7 +1430,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
 
                 uvm_va_space_up_read(va_space);
                 if (mm) {
-                    uvm_up_read_mmap_sem(&mm->mmap_sem);
+                    uvm_up_read_mmap_lock(mm);
                     uvm_va_space_mm_release(va_space);
                     mm = NULL;
                 }
@@ -1449,7 +1449,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
             // TODO: Bug 1867098: Taking mmap_sem here may deadlock between RM
             // and UVM.
             if (mm)
-                uvm_down_read_mmap_sem(&mm->mmap_sem);
+                uvm_down_read_mmap_lock(mm);
 
             uvm_va_space_down_read(va_space);
 
@@ -1555,7 +1555,7 @@ fail:
     if (va_space != NULL) {
         uvm_va_space_up_read(va_space);
         if (mm) {
-            uvm_up_read_mmap_sem(&mm->mmap_sem);
+            uvm_up_read_mmap_lock(mm);
             uvm_va_space_mm_release(va_space);
         }
     }
diff --git a/nvidia-uvm/uvm8_lock.c b/nvidia-uvm/uvm8_lock.c
index f0b3710..e2cd51e 100644
--- a/nvidia-uvm/uvm8_lock.c
+++ b/nvidia-uvm/uvm8_lock.c
@@ -33,7 +33,7 @@ const char *uvm_lock_order_to_string(uvm_lock_order_t lock_order)
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_INVALID);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_GLOBAL);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_ISR);
-        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_MMAP_SEM);
+        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_MMAP_LOCK);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE_READ_ACQUIRE_WRITE_RELEASE_LOCK);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE);
@@ -84,8 +84,8 @@ bool __uvm_record_lock(void *lock, uvm_lock_order_t lock_order, uvm_lock_mode_t
     //       any new invalid uses while we figure out a better way to handle
     //       these dependencies.
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS) {
-        if (test_bit(UVM_LOCK_ORDER_MMAP_SEM, uvm_context->acquired_lock_orders)) {
-            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_sem held\n");
+        if (test_bit(UVM_LOCK_ORDER_MMAP_LOCK, uvm_context->acquired_lock_orders)) {
+            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_lock held\n");
             correct = false;
         }
 
diff --git a/nvidia-uvm/uvm8_lock.h b/nvidia-uvm/uvm8_lock.h
index 60b2403..1aba4b8 100644
--- a/nvidia-uvm/uvm8_lock.h
+++ b/nvidia-uvm/uvm8_lock.h
@@ -47,13 +47,13 @@
 //      Exclusive lock (mutex) per gpu
 //
 //      Protects:
-//      - gpu->isr.replayable_faults.service_lock:
+//      - gpu->parent->isr.replayable_faults.service_lock:
 //        Changes to the state of a GPU as it transitions from top-half to bottom-half
 //        interrupt handler for replayable faults. This lock is acquired for that GPU,
 //        in the ISR top-half. Then a bottom-half is scheduled (to run in a workqueue).
 //        Then the bottom-half releases the lock when that GPU's processing appears to
 //        be done.
-//      - gpu->isr.non_replayable_faults.service_lock:
+//      - gpu->parent->isr.non_replayable_faults.service_lock:
 //        Changes to the state of a GPU in the bottom-half for non-replayable faults.
 //        Non-replayable faults are handed-off from RM instead of directly from the GPU
 //        hardware. This means that we do not keep receiving interrupts after RM pops
@@ -62,36 +62,36 @@
 //        faults ready to be consumed in the buffer, even if there already is some
 //        bottom-half running or scheduled. This lock serializes all scheduled bottom
 //        halves per GPU which service non-replayable faults.
-//      - gpu->isr.access_counters.service_lock:
+//      - gpu->parent->isr.access_counters.service_lock:
 //        Changes to the state of a GPU as it transitions from top-half to bottom-half
 //        interrupt handler for access counter notifications. This lock is acquired for
 //        that GPU, in the ISR top-half. Then a bottom-half is scheduled (to run in a
 //        workqueue). Then the bottom-half releases the lock when that GPU's processing
 //        appears to be done.
 //
-// - mmap_sem
-//      Order: UVM_LOCK_ORDER_MMAP_SEM
+// - mmap_lock (mmap_sem in kernels < 5.8)
+//      Order: UVM_LOCK_ORDER_MMAP_LOCK
 //      Reader/writer lock (rw_semaphore)
 //
-//      We're often called with the kernel already holding mmap_sem: mmap,
+//      We're often called with the kernel already holding mmap_lock: mmap,
 //      munmap, fault, etc. These operations may have to take any number of UVM
-//      locks, so mmap_sem requires special consideration in the lock order,
+//      locks, so mmap_lock requires special consideration in the lock order,
 //      since it's sometimes out of our control.
 //
-//      We need to hold mmap_sem when calling vm_insert_page, which means that
+//      We need to hold mmap_lock when calling vm_insert_page, which means that
 //      any time an operation (such as an ioctl) might need to install a CPU
-//      mapping, it must take current->mm->mmap_sem in read mode very early on.
+//      mapping, it must take current->mm->mmap_lock in read mode very early on.
 //
 //      However, current->mm is not necessarily the owning mm of the UVM vma.
 //      fork or fd passing via a UNIX doman socket can cause that. Notably, this
 //      is also the case when handling GPU faults from a kernel thread. This
-//      means we must lock current->mm->mmap_sem, then look up the UVM vma and
+//      means we must lock current->mm->mmap_lock, then look up the UVM vma and
 //      compare its mm before operating on that vma.
 //
-//      With HMM and ATS, the GPU fault handler takes mmap_sem. GPU faults may
+//      With HMM and ATS, the GPU fault handler takes mmap_lock. GPU faults may
 //      block forward progress of threads holding the RM GPUs lock until those
-//      faults are serviced, which means that mmap_sem cannot be held when the
-//      UVM driver calls into RM. In other words, mmap_sem and the RM GPUs lock
+//      faults are serviced, which means that mmap_lock cannot be held when the
+//      UVM driver calls into RM. In other words, mmap_lock and the RM GPUs lock
 //      are mutually exclusive.
 //
 // - VA space writer serialization lock (va_space->serialize_writers_lock)
@@ -186,8 +186,8 @@
 //      Order: UVM_LOCK_ORDER_VA_SPACE
 //      Reader/writer lock (rw_semaphore) per uvm_va_space (UVM struct file)
 //
-//      This is the UVM equivalent of mmap_sem. It protects all state under that
-//      va_space, such as the VA range tree.
+//      This is the UVM equivalent of mmap_lock. It protects all state under 
+//      that va_space, such as the VA range tree.
 //
 //      Read mode: Faults (CPU and GPU), mapping creation, prefetches. These
 //      will be serialized at the VA block level if necessary. RM calls are
@@ -322,7 +322,7 @@ typedef enum
     UVM_LOCK_ORDER_INVALID = 0,
     UVM_LOCK_ORDER_GLOBAL,
     UVM_LOCK_ORDER_ISR,
-    UVM_LOCK_ORDER_MMAP_SEM,
+    UVM_LOCK_ORDER_MMAP_LOCK,
     UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS,
     UVM_LOCK_ORDER_VA_SPACE_READ_ACQUIRE_WRITE_RELEASE_LOCK,
     UVM_LOCK_ORDER_VA_SPACE,
@@ -408,27 +408,30 @@ bool __uvm_locking_initialized(void);
   // the given mode.
   #define uvm_check_locked(lock, mode) __uvm_check_locked((lock), (lock)->lock_order, (mode))
 
-  // Helpers for recording and asserting mmap_sem state
-  #define uvm_record_lock_mmap_sem_read(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_MODE_SHARED)
+  // Helpers for recording and asserting mmap_lock
+  // (mmap_sem in kernels < 5.8 ) state
+  #define uvm_record_lock_mmap_lock_read(mm) \
+          uvm_record_lock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_MODE_SHARED, false)
+  #define uvm_record_unlock_mmap_lock_read(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_MODE_SHARED, true)
+  #define uvm_record_unlock_mmap_lock_read_out_of_order(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, \
+                                 UVM_LOCK_MODE_SHARED, true)
 
-  #define uvm_record_lock_mmap_sem_write(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_MODE_EXCLUSIVE)
+  #define uvm_record_lock_mmap_lock_write(mm) \
+          uvm_record_lock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_MODE_EXCLUSIVE, false)
+  #define uvm_record_unlock_mmap_lock_write(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_MODE_EXCLUSIVE, true)
+  #define uvm_record_unlock_mmap_lock_write_out_of_order(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, \
+                                UVM_LOCK_MODE_EXCLUSIVE, true)
 
-  #define uvm_check_locked_mmap_sem(mmap_sem, mode) \
-           __uvm_check_locked((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, (mode))
+  #define uvm_check_locked_mmap_lock(mm, flags) \
+           __uvm_check_locked(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, (flags))
 
   // Helpers for recording RM API lock usage around UVM-RM interfaces
   #define uvm_record_lock_rm_api() \
@@ -457,14 +460,14 @@ bool __uvm_locking_initialized(void);
       return false;
   }
 
-  #define uvm_record_lock_mmap_sem_read                 UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read               UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read_out_of_order  UVM_IGNORE_EXPR
-  #define uvm_record_lock_mmap_sem_write                UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write              UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write_out_of_order UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_read                 UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read               UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read_out_of_order  UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_write                UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write              UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write_out_of_order UVM_IGNORE_EXPR
 
-  #define uvm_check_locked_mmap_sem                     uvm_check_locked
+  #define uvm_check_locked_mmap_lock                     uvm_check_locked
 
   #define uvm_record_lock_rm_api()
   #define uvm_record_unlock_rm_api()
@@ -481,45 +484,49 @@ bool __uvm_locking_initialized(void);
 #define uvm_assert_lockable_order(order) UVM_ASSERT(__uvm_check_lockable_order(order))
 #define uvm_assert_unlocked_order(order) UVM_ASSERT(__uvm_check_unlocked_order(order))
 
-// Helpers for locking mmap_sem and recording its usage
-#define uvm_assert_mmap_sem_locked_mode(mmap_sem, mode) ({                          \
-      typeof(mmap_sem) _sem = (mmap_sem);                                           \
-      UVM_ASSERT(rwsem_is_locked(_sem) && uvm_check_locked_mmap_sem(_sem, (mode))); \
-  })
-
-#define uvm_assert_mmap_sem_locked(mmap_sem)        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_MODE_ANY)
-#define uvm_assert_mmap_sem_locked_read(mmap_sem)   uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_MODE_SHARED)
-#define uvm_assert_mmap_sem_locked_write(mmap_sem)  uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_MODE_EXCLUSIVE)
-
-#define uvm_down_read_mmap_sem(mmap_sem) ({             \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_read(_sem);            \
-        down_read(_sem);                                \
+// Helpers for locking mmap_lock (mmap_sem in kernels < 5.8)
+// and recording its usage
+#define uvm_assert_mmap_lock_locked_mode(mm, flags) ({                                      \
+      typeof(mm) _mm = (mm);                                                                \
+      UVM_ASSERT(nv_mm_rwsem_is_locked(_mm) && uvm_check_locked_mmap_lock((_mm), (flags))); \
     })
 
-#define uvm_up_read_mmap_sem(mmap_sem) ({               \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_read(_sem);                                  \
-        uvm_record_unlock_mmap_sem_read(_sem);          \
+#define uvm_assert_mmap_lock_locked(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_MODE_ANY)
+#define uvm_assert_mmap_lock_locked_read(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_MODE_SHARED)
+#define uvm_assert_mmap_lock_locked_write(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_MODE_EXCLUSIVE)
+
+#define uvm_down_read_mmap_lock(mm) ({                  \
+        typeof(mm) _mm = (mm);                          \
+        uvm_record_lock_mmap_lock_read(_mm);            \
+        nv_mmap_read_lock(_mm);                         \
     })
 
-#define uvm_up_read_mmap_sem_out_of_order(mmap_sem) ({      \
-        typeof(mmap_sem) _sem = (mmap_sem);                 \
-        up_read(_sem);                                      \
-        uvm_record_unlock_mmap_sem_read_out_of_order(_sem); \
+#define uvm_up_read_mmap_lock_out_of_order(mm) ({           \
+        typeof(mm) _mm = (mm);                              \
+        nv_mmap_read_unlock(_mm);                           \
+        uvm_record_unlock_mmap_lock_read_out_of_order(_mm); \
     })
 
-#define uvm_down_write_mmap_sem(mmap_sem) ({            \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_write(_sem);           \
-        down_write(_sem);                               \
+#define uvm_up_read_mmap_lock(mm) ({                    \
+        typeof(mm) _mm = (mm);                          \
+        nv_mmap_read_unlock(_mm);                       \
+        uvm_record_unlock_mmap_lock_read(_mm);          \
     })
 
-#define uvm_up_write_mmap_sem(mmap_sem) ({              \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_write(_sem);                                 \
-        uvm_record_unlock_mmap_sem_write(_sem);         \
-    })
+#define uvm_down_write_mmap_lock(mm) ({                 \
+        typeof(mm) _mm = (mm);                          \
+        uvm_record_lock_mmap_lock_write(_mm);           \
+        nv_mmap_write_lock(_mm);                        \
+     })
+
+#define uvm_up_write_mmap_lock(mm) ({                   \
+        typeof(mm) _mm = (mm);                          \
+        nv_mmap_write_unlock(_mm);                      \
+        uvm_record_unlock_mmap_lock_write(_mm);         \
+     })
 
 // Helper for calling a UVM-RM interface function with lock recording
 #define uvm_rm_locked_call(call) ({                     \
@@ -658,6 +665,20 @@ typedef struct
 
 #define uvm_assert_mutex_unlocked(uvm_mutex) UVM_ASSERT(!mutex_is_locked(&(uvm_mutex)->m))
 
+//
+// Linux kernel mutexes cannot be used with interrupts disabled. Doing so
+// can lead to deadlocks.
+// To warn about mutex usages with interrupts disabled, the following
+// macros and inline functions wrap around the raw kernel mutex operations
+// in order to check if the interrupts have been disabled and assert if so.
+//
+// TODO: Bug 2690258: evaluate whether !irqs_disabled() && !in_interrupt() is
+//       enough.
+//
+#define uvm_assert_mutex_interrupts() ({                                                                        \
+        UVM_ASSERT_MSG(!irqs_disabled() && !in_interrupt(), "Mutexes cannot be used with interrupts disabled"); \
+    })
+
 static void uvm_mutex_init(uvm_mutex_t *mutex, uvm_lock_order_t lock_order)
 {
     mutex_init(&mutex->m);
@@ -675,6 +696,7 @@ static void __uvm_mutex_lock(uvm_mutex_t *mutex)
 }
 #define uvm_mutex_lock(mutex) ({                            \
         typeof(mutex) _mutex = (mutex);                     \
+        uvm_assert_mutex_interrupts();                      \
         uvm_record_lock(_mutex, UVM_LOCK_MODE_EXCLUSIVE);   \
         __uvm_mutex_lock(_mutex);                           \
     })
@@ -690,18 +712,23 @@ static void __uvm_mutex_unlock(uvm_mutex_t *mutex)
 }
 #define uvm_mutex_unlock(mutex) ({                          \
         typeof(mutex) _mutex = (mutex);                     \
+        uvm_assert_mutex_interrupts();                      \
         __uvm_mutex_unlock(_mutex);                         \
         uvm_record_unlock(_mutex, UVM_LOCK_MODE_EXCLUSIVE); \
     })
 #define uvm_mutex_unlock_out_of_order(mutex) ({                          \
         typeof(mutex) _mutex = (mutex);                                  \
+        uvm_assert_mutex_interrupts();                                   \
         __uvm_mutex_unlock(_mutex);                                      \
         uvm_record_unlock_out_of_order(_mutex, UVM_LOCK_MODE_EXCLUSIVE); \
     })
 
 // Unlock w/o any tracking. This should be extremely rare and *_no_tracking
 // helpers will be added only as needed.
-#define uvm_mutex_unlock_no_tracking(mutex) mutex_unlock(&(mutex)->m)
+#define uvm_mutex_unlock_no_tracking(mutex) ({  \
+        uvm_assert_mutex_interrupts();          \
+        mutex_unlock(&(mutex)->m);              \
+    })
 
 typedef struct
 {
@@ -720,6 +747,8 @@ static void uvm_sema_init(uvm_semaphore_t *semaphore, int val, uvm_lock_order_t
 #endif
 }
 
+#define uvm_sem_is_locked(uvm_sem) uvm_check_locked(uvm_sem, UVM_LOCK_MODE_SHARED)
+
 static void __uvm_down(uvm_semaphore_t *semaphore)
 {
     down(&semaphore->sem);
@@ -736,11 +765,13 @@ static void __uvm_up(uvm_semaphore_t *semaphore)
 }
 #define uvm_up(sem) ({                                  \
         typeof(sem) _sem = (sem);                       \
+        UVM_ASSERT(uvm_sem_is_locked(_sem));            \
         __uvm_up(_sem);                                 \
         uvm_record_unlock(_sem, UVM_LOCK_MODE_SHARED);  \
     })
 #define uvm_up_out_of_order(sem) ({                                      \
         typeof(sem) _sem = (sem);                                        \
+        UVM_ASSERT(uvm_sem_is_locked(_sem));                             \
         __uvm_up(_sem);                                                  \
         uvm_record_unlock_out_of_order(_sem, UVM_LOCK_MODE_SHARED);      \
     })
diff --git a/nvidia-uvm/uvm8_lock_test.c b/nvidia-uvm/uvm8_lock_test.c
index 967de45..3e186fc 100644
--- a/nvidia-uvm/uvm8_lock_test.c
+++ b/nvidia-uvm/uvm8_lock_test.c
@@ -61,14 +61,14 @@ static bool fake_check_locked(uvm_lock_order_t lock_order, uvm_lock_mode_t mode)
 
 // TODO: Bug 1799173: The lock asserts verify that the RM GPU lock isn't taken
 //       with the VA space lock in exclusive mode, and that the RM GPU lock
-//       isn't taken with mmap_sem held in any mode. Hack around this in the
+//       isn't taken with mmap_lock held in any mode. Hack around this in the
 //       test to enable the checks until we figure out something better.
 static bool skip_lock(uvm_lock_order_t lock_order, uvm_lock_mode_t mode)
 {
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS)
         return mode == UVM_LOCK_MODE_EXCLUSIVE;
 
-    return lock_order == UVM_LOCK_ORDER_MMAP_SEM;
+    return lock_order == UVM_LOCK_ORDER_MMAP_LOCK;
 }
 
 static NV_STATUS test_all_locks_from(uvm_lock_order_t from_lock_order)
diff --git a/nvidia-uvm/uvm8_mem.c b/nvidia-uvm/uvm8_mem.c
index 38b0b2a..48db16d 100644
--- a/nvidia-uvm/uvm8_mem.c
+++ b/nvidia-uvm/uvm8_mem.c
@@ -481,7 +481,7 @@ static NV_STATUS uvm_mem_map_cpu_to_sysmem_user(uvm_mem_t *mem, struct vm_area_s
 
     UVM_ASSERT(uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
 
     // TODO: Bug 1995015: high-order page allocations need to be allocated as
     // compound pages in order to be able to use vm_insert_page on them. This
@@ -505,7 +505,7 @@ static NV_STATUS uvm_mem_map_cpu_to_vidmem_user(uvm_mem_t *mem, struct vm_area_s
     size_t num_chunk_pages = mem->chunk_size / PAGE_SIZE;
 
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     UVM_ASSERT(!uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->backing_gpu != NULL);
     UVM_ASSERT(mem->backing_gpu->numa_info.enabled);
diff --git a/nvidia-uvm/uvm8_migrate.c b/nvidia-uvm/uvm8_migrate.c
index 1df0149..a381f37 100644
--- a/nvidia-uvm/uvm8_migrate.c
+++ b/nvidia-uvm/uvm8_migrate.c
@@ -597,7 +597,7 @@ static NV_STATUS uvm_migrate(uvm_va_space_t *va_space,
     bool is_single_block;
     bool should_do_cpu_preunmap;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(current->mm);
     uvm_assert_rwsem_locked(&va_space->lock);
 
     if (!first_va_range || first_va_range->type != UVM_VA_RANGE_TYPE_MANAGED)
@@ -854,7 +854,7 @@ NV_STATUS uvm_api_migrate(UVM_MIGRATE_PARAMS *params, struct file *filp)
     }
 
     // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (!is_async) {
@@ -936,7 +936,7 @@ done:
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(current->mm);
 
     if (tracker_ptr) {
         if (params->semaphoreAddress && status == NV_OK) {
@@ -982,7 +982,7 @@ NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, st
     uvm_gpu_t *gpu = NULL;
 
     // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->destinationUuid)) {
@@ -1027,7 +1027,7 @@ done:
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(current->mm);
 
     tracker_status = uvm_tracker_wait_deinit(&local_tracker);
     uvm_va_space_up_read(va_space);
diff --git a/nvidia-uvm/uvm8_policy.c b/nvidia-uvm/uvm8_policy.c
index c92350f..186eec8 100644
--- a/nvidia-uvm/uvm8_policy.c
+++ b/nvidia-uvm/uvm8_policy.c
@@ -39,7 +39,7 @@ bool uvm_is_valid_vma_range(NvU64 start, NvU64 length)
     const NvU64 end = start + length;
     struct vm_area_struct *vma;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(current->mm);
 
     vma = find_vma_intersection(current->mm, start, end);
 
@@ -234,7 +234,7 @@ NV_STATUS uvm_api_set_preferred_location(const UVM_SET_PREFERRED_LOCATION_PARAMS
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
     has_va_space_write_lock = true;
 
@@ -307,7 +307,7 @@ done:
         uvm_va_space_up_write(va_space);
     else
         uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status == NV_OK ? tracker_status : status;
 }
@@ -319,7 +319,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, params->requestedBase, params->length);
@@ -330,7 +330,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
         status = NV_OK;
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
@@ -411,7 +411,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -475,7 +475,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status;
 }
@@ -658,7 +658,7 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
     UVM_ASSERT(va_space);
 
     // We need mmap_sem as we may create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -712,7 +712,8 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
+
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_populate_pageable.c b/nvidia-uvm/uvm8_populate_pageable.c
index a3afd30..ff4b912 100644
--- a/nvidia-uvm/uvm8_populate_pageable.c
+++ b/nvidia-uvm/uvm8_populate_pageable.c
@@ -49,7 +49,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     UVM_ASSERT(PAGE_ALIGNED(outer));
     UVM_ASSERT(vma->vm_end > start);
     UVM_ASSERT(vma->vm_start < outer);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     if (!min_prot_ok)
         return NV_ERR_INVALID_ADDRESS;
@@ -67,12 +67,12 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     // page fault handler. The page fault is caused by get_user_pages.
     uvm_managed_vma = uvm_file_is_nvidia_uvm(vma->vm_file);
     if (uvm_managed_vma)
-        uvm_record_unlock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_read(mm);
 
     ret = NV_GET_USER_PAGES(start, vma_num_pages, is_writable, 0, NULL, NULL);
 
     if (uvm_managed_vma)
-        uvm_record_lock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_lock_mmap_lock_read(mm);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
@@ -95,7 +95,7 @@ NV_STATUS uvm_populate_pageable(struct mm_struct *mm,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     vma = find_vma_intersection(mm, start, outer);
 
@@ -156,14 +156,14 @@ NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params,
 
     // mmap_sem is needed to traverse the vmas in the input range and call into
     // get_user_pages
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
 
     if (allow_managed || uvm_va_space_range_empty(va_space, params->base, params->base + params->length - 1))
         status = uvm_populate_pageable(current->mm, params->base, params->length, min_prot);
     else
         status = NV_ERR_INVALID_ADDRESS;
 
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status;
 }
diff --git a/nvidia-uvm/uvm8_tools.c b/nvidia-uvm/uvm8_tools.c
index 727f881..ae0744f 100644
--- a/nvidia-uvm/uvm8_tools.c
+++ b/nvidia-uvm/uvm8_tools.c
@@ -264,9 +264,9 @@ static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct p
         goto fail;
     }
 
-    down_read(&current->mm->mmap_sem);
+    nv_mmap_read_lock(current->mm);
     ret = NV_GET_USER_PAGES(user_va, num_pages, 1, 0, *pages, vmas);
-    up_read(&current->mm->mmap_sem);
+    nv_mmap_read_unlock(current->mm);
     if (ret != num_pages) {
         status = NV_ERR_INVALID_ARGUMENT;
         goto fail;
diff --git a/nvidia-uvm/uvm8_va_block.c b/nvidia-uvm/uvm8_va_block.c
index f1430a3..a52e6e1 100644
--- a/nvidia-uvm/uvm8_va_block.c
+++ b/nvidia-uvm/uvm8_va_block.c
@@ -6417,7 +6417,7 @@ static NV_STATUS block_map_cpu_page_to(uvm_va_block_t *block,
     // us, so we can safely operate on the vma but we can't use
     // uvm_va_range_vma_current.
     vma = uvm_va_range_vma(va_range);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
 
     // Add the mapping
     addr = uvm_va_block_cpu_page_address(block, page_index);
@@ -10479,7 +10479,7 @@ NV_STATUS uvm8_test_change_pte_mapping(UVM_TEST_CHANGE_PTE_MAPPING_PARAMS *param
 
     // mmap_sem isn't needed for invalidating CPU mappings, but it will be
     // needed for inserting them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->uuid)) {
@@ -10562,7 +10562,7 @@ out_block:
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_va_block_context_free(block_context);
 
@@ -10577,7 +10577,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
     BUILD_BUG_ON(UVM_TEST_VA_BLOCK_SIZE != UVM_VA_BLOCK_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     status = uvm_va_block_find(va_space, params->lookup_address, &va_block);
@@ -10595,7 +10595,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
@@ -10611,7 +10611,7 @@ NV_STATUS uvm8_test_va_residency_info(UVM_TEST_VA_RESIDENCY_INFO_PARAMS *params,
     unsigned release_block_count = 0;
     NvU64 addr = UVM_ALIGN_DOWN(params->lookup_address, PAGE_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     status = uvm_va_block_find(va_space, addr, &block);
@@ -10736,7 +10736,7 @@ out:
             uvm_va_block_release(block);
     }
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.c b/nvidia-uvm/uvm8_va_range.c
index 3597c19..2c88bb1 100644
--- a/nvidia-uvm/uvm8_va_range.c
+++ b/nvidia-uvm/uvm8_va_range.c
@@ -1764,7 +1764,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
     va_space = uvm_va_space_get(filp);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, params->lookup_address);
@@ -1825,7 +1825,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.h b/nvidia-uvm/uvm8_va_range.h
index cd90679..8a22362 100644
--- a/nvidia-uvm/uvm8_va_range.h
+++ b/nvidia-uvm/uvm8_va_range.h
@@ -676,7 +676,7 @@ static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, s
     if (mm != vma->vm_mm)
         return NULL;
 
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     return vma;
 }
 
diff --git a/nvidia-uvm/uvm8_va_space.c b/nvidia-uvm/uvm8_va_space.c
index d39c131..40b4fc2 100644
--- a/nvidia-uvm/uvm8_va_space.c
+++ b/nvidia-uvm/uvm8_va_space.c
@@ -474,7 +474,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
     if (flags & ~UVM_INIT_FLAGS_MASK)
         return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_write_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     if (atomic_read(&va_space->initialized)) {
@@ -503,7 +503,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_write_mmap_lock(current->mm);
 
     return status;
 }
@@ -767,7 +767,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
 
     // The mmap_sem lock is needed to establish CPU mappings to any pages
     // evicted from the GPU if accessed by CPU is set for them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -784,7 +784,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
     uvm_processor_mask_clear(&va_space->gpu_unregister_in_progress, gpu->id);
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_deferred_free_object_list(&deferred_free_list);
 
@@ -1242,9 +1242,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     // need mmap_sem in read mode to handle potential CPU mapping changes in
     // uvm_va_range_add_gpu_va_space().
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_write_mmap_lock(current->mm);
     else
-        uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_read_mmap_lock(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -1299,9 +1299,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(current->mm);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(current->mm);
 
     uvm_gpu_release(gpu);
     return NV_OK;
@@ -1320,9 +1320,9 @@ error:
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(current->mm);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(current->mm);
 
     destroy_gpu_va_space(gpu_va_space);
 
@@ -1396,7 +1396,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     uvm_gpu_retain(gpu);
     uvm_va_space_up_read_rm(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     // We dropped the lock so we have to re-verify that this gpu_va_space is
@@ -1414,7 +1414,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     }
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_deferred_free_object_list(&deferred_free_list);
     uvm_gpu_va_space_release(gpu_va_space);
diff --git a/nvidia-uvm/uvm8_va_space_mm.c b/nvidia-uvm/uvm8_va_space_mm.c
index 8ce53f6..bd1eac7 100644
--- a/nvidia-uvm/uvm8_va_space_mm.c
+++ b/nvidia-uvm/uvm8_va_space_mm.c
@@ -238,7 +238,7 @@ bool uvm_va_space_mm_enabled(uvm_va_space_t *va_space)
     static int uvm_mmu_notifier_register(uvm_va_space_mm_t *va_space_mm)
     {
         UVM_ASSERT(va_space_mm->mm);
-        uvm_assert_mmap_sem_locked_write(&va_space_mm->mm->mmap_sem);
+        uvm_assert_mmap_lock_locked_write(va_space_mm->mm);
 
         if (UVM_ATS_IBM_SUPPORTED_IN_DRIVER() && g_uvm_global.ats.enabled)
             va_space_mm->mmu_notifier.ops = &uvm_mmu_notifier_ops_ats;
@@ -270,7 +270,7 @@ NV_STATUS uvm_va_space_mm_register(uvm_va_space_t *va_space)
     uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
     int ret;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(current->mm);
     uvm_assert_rwsem_locked_write(&va_space->lock);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) != NV_OK);
@@ -305,7 +305,7 @@ void uvm_va_space_mm_unregister(uvm_va_space_t *va_space)
     // mmu_notifier_unregister() may trigger uvm_va_space_mm_shutdown(), which
     // takes those locks and also waits for other threads which may take those
     // locks.
-    uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_SEM);
+    uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_LOCK);
     uvm_assert_unlocked_order(UVM_LOCK_ORDER_VA_SPACE);
 
     if (!va_space_mm->mm)
@@ -538,9 +538,9 @@ static NV_STATUS mm_read64(struct mm_struct *mm, NvU64 addr, NvU64 *val)
 
     UVM_ASSERT(IS_ALIGNED(addr, sizeof(val)));
 
-    uvm_down_read_mmap_sem(&mm->mmap_sem);
+    uvm_down_read_mmap_lock(mm);
     ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)addr, 1, write, force, &page, NULL);
-    uvm_up_read_mmap_sem(&mm->mmap_sem);
+    uvm_up_read_mmap_lock(mm);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
diff --git a/nvidia/linux_nvswitch.c b/nvidia/linux_nvswitch.c
index 7089747..5f6d37a 100644
--- a/nvidia/linux_nvswitch.c
+++ b/nvidia/linux_nvswitch.c
@@ -27,6 +27,7 @@
 #include "conftest.h"
 #include "nvmisc.h"
 #include "nv-linux.h"
+#include "nv-time.h"
 #include "nvlink_common.h"
 #include "nvlink_errors.h"
 #include "nv-kthread-q.h"
diff --git a/nvidia/nvidia.Kbuild b/nvidia/nvidia.Kbuild
index 34fb4dc..4371d78 100644
--- a/nvidia/nvidia.Kbuild
+++ b/nvidia/nvidia.Kbuild
@@ -189,7 +189,9 @@ NV_CONFTEST_TYPE_COMPILE_TESTS += vm_fault_has_address
 NV_CONFTEST_TYPE_COMPILE_TESTS += backlight_properties_type
 NV_CONFTEST_TYPE_COMPILE_TESTS += vmbus_channel_has_ringbuffer_page
 NV_CONFTEST_TYPE_COMPILE_TESTS += proc_ops
+NV_CONFTEST_TYPE_COMPILE_TESTS += vmalloc_has_pgprot_t_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += timeval
+NV_CONFTEST_TYPE_COMPILE_TESTS += mm_has_mmap_lock
 NV_CONFTEST_TYPE_COMPILE_TESTS += kmem_cache_has_kobj_remove_work
 NV_CONFTEST_TYPE_COMPILE_TESTS += sysfs_slab_unlink
 
diff --git a/nvidia/nvlink_linux.c b/nvidia/nvlink_linux.c
index dbe2091..613e235 100644
--- a/nvidia/nvlink_linux.c
+++ b/nvidia/nvlink_linux.c
@@ -269,7 +269,6 @@ static long nvlink_fops_unlocked_ioctl(struct file *file,
     return nvlink_fops_ioctl(NV_FILE_INODE(file), file, cmd, arg);
 }
 
-
 static const struct file_operations nvlink_fops = {
     .owner           = THIS_MODULE,
     .open            = nvlink_fops_open,
diff --git a/nvidia/os-mlock.c b/nvidia/os-mlock.c
index 3074f77..19f87d9 100644
--- a/nvidia/os-mlock.c
+++ b/nvidia/os-mlock.c
@@ -44,7 +44,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
 
     vma = find_vma(mm, (NvUPtr)address);
     if ((vma == NULL) || ((vma->vm_flags & (VM_IO | VM_PFNMAP)) == 0))
@@ -77,7 +77,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
     }
 
 done:
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
 
     return rmStatus;
 #else
@@ -115,10 +115,10 @@ NV_STATUS NV_API_CALL os_lock_user_pages(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
     ret = NV_GET_USER_PAGES((unsigned long)address,
                             page_count, write, force, user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
     pinned = ret;
 
     if (ret < 0)
-- 
2.25.1

