From f77bbdcae7474345a7e0b7be15340bc6f21a17af Mon Sep 17 00:00:00 2001
From: Alberto Milone <alberto.milone@canonical.com>
Date: Thu, 9 Jul 2020 11:28:21 +0200
Subject: [PATCH 1/1] Add support for Linux 5.8

---
 common/inc/nv-linux.h                       |   4 +
 common/inc/nv-mm.h                          |  54 ++++++
 common/inc/nv-procfs.h                      |  54 +++++-
 conftest.sh                                 |  88 ++++++++++
 nvidia-drm/nvidia-drm-linux.c               |   4 +-
 nvidia-drm/nvidia-drm.Kbuild                |   1 +
 nvidia-uvm/uvm8.c                           |  48 +++---
 nvidia-uvm/uvm8_ats_faults.h                |   2 +-
 nvidia-uvm/uvm8_gpu_access_counters.c       |   4 +-
 nvidia-uvm/uvm8_gpu_non_replayable_faults.c |   4 +-
 nvidia-uvm/uvm8_gpu_replayable_faults.c     |   6 +-
 nvidia-uvm/uvm8_lock.c                      |   6 +-
 nvidia-uvm/uvm8_lock.h                      | 176 ++++++++++++--------
 nvidia-uvm/uvm8_lock_test.c                 |   4 +-
 nvidia-uvm/uvm8_mem.c                       |   4 +-
 nvidia-uvm/uvm8_migrate.c                   |  10 +-
 nvidia-uvm/uvm8_policy.c                    |  19 ++-
 nvidia-uvm/uvm8_populate_pageable.c         |  12 +-
 nvidia-uvm/uvm8_tools.c                     |   4 +-
 nvidia-uvm/uvm8_va_block.c                  |  14 +-
 nvidia-uvm/uvm8_va_range.c                  |   4 +-
 nvidia-uvm/uvm8_va_range.h                  |   2 +-
 nvidia-uvm/uvm8_va_space.c                  |  24 +--
 nvidia-uvm/uvm8_va_space_mm.c               |  10 +-
 nvidia/linux_nvswitch.c                     |   1 +
 nvidia/nvidia.Kbuild                        |   2 +
 nvidia/nvlink_linux.c                       |   1 -
 nvidia/os-mlock.c                           |   8 +-
 28 files changed, 402 insertions(+), 168 deletions(-)

diff --git a/common/inc/nv-linux.h b/common/inc/nv-linux.h
index 2def6ca..9c747b0 100644
--- a/common/inc/nv-linux.h
+++ b/common/inc/nv-linux.h
@@ -509,7 +509,11 @@ extern NvBool nvos_is_chipset_io_coherent(void);
 
 static inline void *nv_vmalloc(unsigned long size)
 {
+#if defined(NV_VMALLOC_HAS_PGPROT_T_ARG)
     void *ptr = __vmalloc(size, GFP_KERNEL, PAGE_KERNEL);
+#else
+    void *ptr = __vmalloc(size, GFP_KERNEL);
+#endif
     if (ptr)
         NV_MEMDBG_ADD(ptr, size);
     return ptr;
diff --git a/common/inc/nv-mm.h b/common/inc/nv-mm.h
index 4d75de0..d402715 100644
--- a/common/inc/nv-mm.h
+++ b/common/inc/nv-mm.h
@@ -201,4 +201,58 @@ static inline unsigned long nv_page_fault_va(struct vm_fault *vmf)
 #endif
 }
 
+static inline void nv_mmap_read_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_read_lock(mm);
+#else
+    down_read(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_read_unlock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_read_unlock(mm);
+#else
+    up_read(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_write_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_write_lock(mm);
+#else
+    down_write(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_write_unlock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_write_unlock(mm);
+#else
+    up_write(&mm->mmap_sem);
+#endif
+}
+
+static inline int nv_mm_rwsem_is_locked(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    return rwsem_is_locked(&mm->mmap_lock);
+#else
+    return rwsem_is_locked(&mm->mmap_sem);
+#endif
+}
+
+static inline struct rw_semaphore *nv_mmap_get_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    return &mm->mmap_lock;
+#else
+    return &mm->mmap_sem;
+#endif
+}
+
 #endif // __NV_MM_H__
diff --git a/common/inc/nv-procfs.h b/common/inc/nv-procfs.h
index 7534c58..33bb4bb 100644
--- a/common/inc/nv-procfs.h
+++ b/common/inc/nv-procfs.h
@@ -50,6 +50,18 @@ typedef struct file_operations nv_proc_ops_t;
 #define NV_PROC_OPS_RELEASE release
 #endif
 
+#if defined(NV_PROCFS_PROC_OPS_PRESENT)
+#define NV_CREATE_PROC_FILE(filename,parent,__name,__data)               \
+   ({                                                                    \
+        struct proc_dir_entry *__entry;                                  \
+        int mode = (S_IFREG | S_IRUGO);                                  \
+        const struct proc_ops *fops = &nv_procfs_##__name##_fops;		 \
+        if (fops->proc_write != 0)                                            \
+            mode |= S_IWUSR;                                             \
+        __entry = proc_create_data(filename, mode, parent, fops, __data);\
+        __entry;                                                         \
+    })
+#else
 #define NV_CREATE_PROC_FILE(filename,parent,__name,__data)               \
    ({                                                                    \
         struct proc_dir_entry *__entry;                                  \
@@ -60,6 +72,7 @@ typedef struct file_operations nv_proc_ops_t;
         __entry = proc_create_data(filename, mode, parent, fops, __data);\
         __entry;                                                         \
     })
+#endif
 
 /*
  * proc_mkdir_mode exists in Linux 2.6.9, but isn't exported until Linux 3.0.
@@ -99,6 +112,45 @@ typedef struct file_operations nv_proc_ops_t;
     remove_proc_entry(entry->name, entry->parent);
 #endif
 
+#if defined(NV_PROCFS_PROC_OPS_PRESENT)
+#define NV_DEFINE_SINGLE_PROCFS_FILE(name, open_callback, close_callback)     \
+    static int nv_procfs_open_##name(                                         \
+        struct inode *inode,                                                  \
+        struct file *filep                                                    \
+    )                                                                         \
+    {                                                                         \
+        int ret;                                                              \
+        ret = single_open(filep, nv_procfs_read_##name,                       \
+                          NV_PDE_DATA(inode));                                \
+        if (ret < 0)                                                          \
+        {                                                                     \
+            return ret;                                                       \
+        }                                                                     \
+        ret = open_callback();                                                \
+        if (ret < 0)                                                          \
+        {                                                                     \
+            single_release(inode, filep);                                     \
+        }                                                                     \
+        return ret;                                                           \
+    }                                                                         \
+                                                                              \
+    static int nv_procfs_release_##name(                                      \
+        struct inode *inode,                                                  \
+        struct file *filep                                                    \
+    )                                                                         \
+    {                                                                         \
+        close_callback();                                                     \
+        return single_release(inode, filep);                                  \
+    }                                                                         \
+                                                                              \
+    static const struct proc_ops nv_procfs_##name##_fops = {           \
+        .proc_open       = nv_procfs_open_##name,                                  \
+        .proc_read       = seq_read,                                               \
+        .proc_lseek     = seq_lseek,                                              \
+        .proc_release    = nv_procfs_release_##name,                               \
+    };
+
+#else
 #define NV_DEFINE_SINGLE_PROCFS_FILE(name, open_callback, close_callback)     \
     static int nv_procfs_open_##name(                                         \
         struct inode *inode,                                                  \
@@ -136,7 +188,7 @@ typedef struct file_operations nv_proc_ops_t;
         .NV_PROC_OPS_LSEEK   = seq_lseek,                                     \
         .NV_PROC_OPS_RELEASE = nv_procfs_release_##name,                      \
     };
-
+#endif
 #endif  /* CONFIG_PROC_FS */
 
 #endif /* _NV_PROCFS_H */
diff --git a/conftest.sh b/conftest.sh
index 4545492..66b5753 100755
--- a/conftest.sh
+++ b/conftest.sh
@@ -1278,6 +1278,40 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_PROC_REMOVE_PRESENT" "" "functions"
         ;;
 
+        proc_create)
+            #
+            # Determine if the proc_*() function rely on file_operations.
+            #
+            # Added by commit a8ca16ea7b0a ("proc: Supply a function to
+            # Replaced by commit 97a32539b956 ("proc: convert everything
+            # to "struct proc_ops"
+            #
+            CODE="
+            #include <linux/proc_fs.h>
+            #include <linux/seq_file.h>
+
+            static int conftest_proc_show(struct seq_file *m, void *v) {
+                return 0;
+            }
+
+            static int conftest_proc_open(struct inode *inode, struct  file *file) {
+              return single_open(file, conftest_proc_show, NULL);
+            }
+
+            static const struct proc_ops conftest_proc_ops = {
+                .proc_open = conftest_proc_open,
+                .proc_read = seq_read,
+                .proc_lseek = seq_lseek,
+                .proc_release = single_release,
+            };
+
+            void conftest_proc_create(void) {
+                proc_create(\"conftest_proc\", 0, NULL, &conftest_proc_ops);
+            }"
+
+            compile_check_conftest "$CODE" "NV_PROCFS_PROC_OPS_PRESENT" "" "types"
+        ;;
+
         backing_dev_info)
             #
             # Determine if the 'address_space' structure has
@@ -2928,6 +2962,22 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_TIMER_SETUP_PRESENT" "" "functions"
         ;;
 
+        timeval_structs)
+            #
+            # Determine if timespec and timeval structs are present.
+            #
+            # Hidden by commit c766d1472c70 ("y2038: hide timeval/timespec/itimerval/
+            # itimerspec types") (2020-02-20)
+            #
+            CODE="
+            #include <linux/time.h>
+            void conftest_timeval_structs(void) {
+                struct timeval tmp;
+            }"
+            compile_check_conftest "$CODE" "NV_TIME_STRUCTS_PRESENT" "" "types"
+        ;;
+
+
         radix_tree_replace_slot)
             #
             # Determine if the radix_tree_replace_slot() function is
@@ -3645,6 +3695,44 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_KTIME_GET_REAL_TS64_PRESENT" "" "functions"
         ;;
 
+        vmalloc_has_pgprot_t_arg)
+            #
+            # Determine if __vmalloc has the 'pgprot' argument.
+            #
+            # The third argument to __vmalloc, page protection
+            # 'pgprot_t prot', was removed by commit 88dca4ca5a93
+            # (mm: remove the pgprot argument to __vmalloc)
+            # in v5.8-rc1 (2020-06-01).
+        CODE="
+        #include <linux/vmalloc.h>
+
+        void conftest_vmalloc_has_pgprot_t_arg(void) {
+            pgprot_t prot;
+            (void)__vmalloc(0, 0, prot);
+        }"
+
+            compile_check_conftest "$CODE" "NV_VMALLOC_HAS_PGPROT_T_ARG" "" "types"
+
+        ;;
+
+        mm_has_mmap_lock)
+            #
+            # Determine if the 'mm_struct' structure has a 'mmap_lock' field.
+            #
+            # Kernel commit da1c55f1b272 ("mmap locking API: rename mmap_sem
+            # to mmap_lock") replaced the field 'mmap_sem' by 'mmap_lock'
+            # in v5.8-rc1 (2020-06-08).
+            CODE="
+            #include <linux/mm_types.h>
+
+            int conftest_mm_has_mmap_lock(void) {
+                return offsetof(struct mm_struct, mmap_lock);
+            }"
+
+            compile_check_conftest "$CODE" "NV_MM_HAS_MMAP_LOCK" "" "types"
+
+        ;;
+
         # When adding a new conftest entry, please use the correct format for
         # specifying the relevant upstream Linux kernel commit.
         #
diff --git a/nvidia-drm/nvidia-drm-linux.c b/nvidia-drm/nvidia-drm-linux.c
index 1d3e658..f8fcde4 100644
--- a/nvidia-drm/nvidia-drm-linux.c
+++ b/nvidia-drm/nvidia-drm-linux.c
@@ -103,11 +103,11 @@ int nv_drm_lock_user_pages(unsigned long address,
         return -ENOMEM;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
 
     pages_pinned = NV_GET_USER_PAGES(address, pages_count, write, force,
                                      user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
 
     if (pages_pinned < 0 || (unsigned)pages_pinned < pages_count) {
         goto failed;
diff --git a/nvidia-drm/nvidia-drm.Kbuild b/nvidia-drm/nvidia-drm.Kbuild
index 4d4c90e..901a783 100644
--- a/nvidia-drm/nvidia-drm.Kbuild
+++ b/nvidia-drm/nvidia-drm.Kbuild
@@ -90,3 +90,4 @@ NV_CONFTEST_TYPE_COMPILE_TESTS += drm_atomic_helper_swap_state_has_stall_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_driver_prime_flag_present
 NV_CONFTEST_TYPE_COMPILE_TESTS += vm_fault_t
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_gem_object_has_resv
+NV_CONFTEST_TYPE_COMPILE_TESTS += mm_has_mmap_lock
diff --git a/nvidia-uvm/uvm8.c b/nvidia-uvm/uvm8.c
index 26b4378..14289f8 100644
--- a/nvidia-uvm/uvm8.c
+++ b/nvidia-uvm/uvm8.c
@@ -367,8 +367,9 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
         return;
     }
 
-    // At this point we are guaranteed that the mmap_sem is held in write mode.
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    // At this point we are guaranteed that the mmap_lock is held in write
+    // mode.
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     // Split vmas should always fall entirely within the old one, and be on one
     // side.
@@ -417,7 +418,7 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_open_managed_entry(struct vm_area_struct *vma)
@@ -432,7 +433,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     bool make_zombie = false;
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(current->mm);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) == NV_OK);
 
@@ -460,7 +461,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
         }
     }
 
-    // See uvm_mmap for why we need this in addition to mmap_sem
+    // See uvm_mmap for why we need this in addition to mmap_lock
     uvm_va_space_down_write(va_space);
 
     uvm_destroy_vma_managed(vma, make_zombie);
@@ -476,7 +477,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     uvm_va_space_up_write(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_close_managed_entry(struct vm_area_struct *vma)
@@ -521,10 +522,10 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 
     service_context->cpu_fault.wakeup_time_stamp = 0;
 
-    // The mmap_sem might be held in write mode, but the mode doesn't matter for
-    // the purpose of lock ordering and we don't rely on it being in write
+    // The mmap_lock might be held in write mode, but the mode doesn't matter
+    // for the purpose of lock ordering and we don't rely on it being in write
     // anywhere so just record it as read mode in all cases.
-    uvm_record_lock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_lock_mmap_lock_read(vma->vm_mm);
 
     do {
         bool do_sleep = false;
@@ -587,7 +588,7 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
     }
 
     uvm_va_space_up_read(va_space);
-    uvm_record_unlock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_read(vma->vm_mm);
 
     if (status == NV_OK) {
         status = uvm_global_mask_check_ecc_error(&gpus_to_check_for_ecc);
@@ -663,7 +664,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
     bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
     NV_STATUS status;
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -703,7 +704,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
 
     uvm_va_space_up_write(va_space);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_open_semaphore_pool_entry(struct vm_area_struct *vma)
@@ -718,7 +719,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(current->mm);
 
     uvm_va_space_down_read(va_space);
 
@@ -727,7 +728,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_up_read(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_close_semaphore_pool_entry(struct vm_area_struct *vma)
@@ -762,12 +763,13 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
     if (status != NV_OK)
         return -EBADFD;
 
-    // TODO: Bug 2667195: Fail with EOPNOTSUPP if the VA space is associated
-    //       with an mm, but current->mm doesn't match that mm. This is work
-    //       associated with bug 2667197, but it doesn't make sense to handle
-    //       this until the mm association is moved to VA space init. Until that
-    //       happens, the mm association could happen at UvmRegisterGpuVaSpace,
-    //       which may be too late to do this check.
+    // When the VA space is associated with an mm, all vmas under the VA space
+    // must come from that mm.
+    if (uvm_va_space_mm_enabled(va_space)) {
+        UVM_ASSERT(va_space->va_space_mm.mm);
+        if (va_space->va_space_mm.mm != current->mm)
+            return -EOPNOTSUPP;
+    }
 
     // UVM mappings are required to set offset == VA. This simplifies things
     // since we don't have to worry about address aliasing (except for fork,
@@ -796,7 +798,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
         return 0;
     }
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     // VM_MIXEDMAP      Required to use vm_insert_page
     //
@@ -822,7 +824,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
     }
     vma_wrapper_allocated = true;
 
-    // The kernel has taken mmap_sem in write mode, but that doesn't prevent
+    // The kernel has taken mmap_lock in write mode, but that doesn't prevent
     // this va_space from being modified by the GPU fault path or from the ioctl
     // path where we don't have this mm for sure, so we have to lock the VA
     // space directly.
@@ -863,7 +865,7 @@ out:
     if (ret != 0 && vma_wrapper_allocated)
         uvm_vma_wrapper_destroy(vma->vm_private_data);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 
     uvm_up_read(&g_uvm_global.pm.lock);
 
diff --git a/nvidia-uvm/uvm8_ats_faults.h b/nvidia-uvm/uvm8_ats_faults.h
index e001b60..1d10534 100644
--- a/nvidia-uvm/uvm8_ats_faults.h
+++ b/nvidia-uvm/uvm8_ats_faults.h
@@ -39,7 +39,7 @@ NV_STATUS uvm_ats_invalidate_tlbs(uvm_gpu_va_space_t *gpu_va_space,
 static bool uvm_can_ats_service_faults(uvm_gpu_va_space_t *gpu_va_space, struct mm_struct *mm)
 {
     if (mm)
-        uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+        uvm_assert_mmap_lock_locked(mm);
     if (gpu_va_space->ats.enabled)
         UVM_ASSERT(g_uvm_global.ats.enabled);
 
diff --git a/nvidia-uvm/uvm8_gpu_access_counters.c b/nvidia-uvm/uvm8_gpu_access_counters.c
index c567a0f..47c56c0 100644
--- a/nvidia-uvm/uvm8_gpu_access_counters.c
+++ b/nvidia-uvm/uvm8_gpu_access_counters.c
@@ -1180,7 +1180,7 @@ static NV_STATUS service_phys_single_va_block(uvm_gpu_t *gpu,
         // in order to lock it before locking the VA space.
         mm = uvm_va_space_mm_retain(va_space);
         if (mm)
-            uvm_down_read_mmap_sem(&mm->mmap_sem);
+            uvm_down_read_mmap_lock(mm);
 
         // Re-check that the VA block is valid after taking the VA space lock
         uvm_va_space_down_read(va_space);
@@ -1222,7 +1222,7 @@ done:
         uvm_va_space_up_read(va_space);
 
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(mm);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
index 81b5626..9682154 100644
--- a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
@@ -511,7 +511,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
     // TODO: Bug 1867098: Taking mmap_sem here may deadlock between RM
     // and UVM.
     if (mm)
-        uvm_down_read_mmap_sem(&mm->mmap_sem);
+        uvm_down_read_mmap_lock(mm);
 
     uvm_va_space_down_read(va_space);
 
@@ -562,7 +562,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
 exit_no_channel:
     uvm_va_space_up_read(va_space);
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(mm);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_replayable_faults.c b/nvidia-uvm/uvm8_gpu_replayable_faults.c
index 1200aae..7e5120d 100644
--- a/nvidia-uvm/uvm8_gpu_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_replayable_faults.c
@@ -1464,7 +1464,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
 
                 uvm_va_space_up_read(va_space);
                 if (mm) {
-                    uvm_up_read_mmap_sem(&mm->mmap_sem);
+                    uvm_up_read_mmap_lock(mm);
                     uvm_va_space_mm_release(va_space);
                     mm = NULL;
                 }
@@ -1483,7 +1483,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
             // TODO: Bug 1867098: Taking mmap_sem here may deadlock between RM
             // and UVM.
             if (mm)
-                uvm_down_read_mmap_sem(&mm->mmap_sem);
+                uvm_down_read_mmap_lock(mm);
 
             uvm_va_space_down_read(va_space);
 
@@ -1590,7 +1590,7 @@ fail:
     if (va_space != NULL) {
         uvm_va_space_up_read(va_space);
         if (mm) {
-            uvm_up_read_mmap_sem(&mm->mmap_sem);
+            uvm_up_read_mmap_lock(mm);
             uvm_va_space_mm_release(va_space);
         }
     }
diff --git a/nvidia-uvm/uvm8_lock.c b/nvidia-uvm/uvm8_lock.c
index bf53eda..c10cd27 100644
--- a/nvidia-uvm/uvm8_lock.c
+++ b/nvidia-uvm/uvm8_lock.c
@@ -34,7 +34,7 @@ const char *uvm_lock_order_to_string(uvm_lock_order_t lock_order)
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_GLOBAL_PM);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_GLOBAL);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_ISR);
-        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_MMAP_SEM);
+        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_MMAP_LOCK);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACES_LIST);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE_READ_ACQUIRE_WRITE_RELEASE_LOCK);
@@ -88,8 +88,8 @@ bool __uvm_record_lock(void *lock, uvm_lock_order_t lock_order, uvm_lock_flags_t
     //       any new invalid uses while we figure out a better way to handle
     //       these dependencies.
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS) {
-        if (test_bit(UVM_LOCK_ORDER_MMAP_SEM, uvm_context->acquired_lock_orders)) {
-            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_sem held\n");
+        if (test_bit(UVM_LOCK_ORDER_MMAP_LOCK, uvm_context->acquired_lock_orders)) {
+            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_lock held\n");
             correct = false;
         }
 
diff --git a/nvidia-uvm/uvm8_lock.h b/nvidia-uvm/uvm8_lock.h
index 18d129d..bc1d275 100644
--- a/nvidia-uvm/uvm8_lock.h
+++ b/nvidia-uvm/uvm8_lock.h
@@ -44,7 +44,7 @@
 //      sleep cycles.
 //
 //      This lock is special: while it's taken by user-facing entry points,
-//      and may be taken before or after mmap_sem, this apparent violation of
+//      and may be taken before or after mmap_lock, this apparent violation of
 //      lock ordering is permissible because pm_lock may only be taken via
 //      trylock in read mode by paths which already hold any lower-level
 //      locks, as well as by paths subject to the kernel's freezer.  Paths
@@ -54,7 +54,7 @@
 //      infrequently, and only as part of to power management.  Starvation is
 //      not a concern.
 //
-//      The mmap_sem deadlock potential aside, the trylock approch is also
+//      The mmap_lock deadlock potential aside, the trylock approch is also
 //      motivated by the need to prevent user threads making UVM system calls
 //      from blocking when UVM is suspended: when the kernel suspends the
 //      system, the freezer employed to stop user tasks requires these tasks
@@ -74,13 +74,13 @@
 //      Exclusive lock (mutex) per gpu
 //
 //      Protects:
-//      - gpu->isr.replayable_faults.service_lock:
+//      - gpu->parent->isr.replayable_faults.service_lock:
 //        Changes to the state of a GPU as it transitions from top-half to bottom-half
 //        interrupt handler for replayable faults. This lock is acquired for that GPU,
 //        in the ISR top-half. Then a bottom-half is scheduled (to run in a workqueue).
 //        Then the bottom-half releases the lock when that GPU's processing appears to
 //        be done.
-//      - gpu->isr.non_replayable_faults.service_lock:
+//      - gpu->parent->isr.non_replayable_faults.service_lock:
 //        Changes to the state of a GPU in the bottom-half for non-replayable faults.
 //        Non-replayable faults are handed-off from RM instead of directly from the GPU
 //        hardware. This means that we do not keep receiving interrupts after RM pops
@@ -89,36 +89,36 @@
 //        faults ready to be consumed in the buffer, even if there already is some
 //        bottom-half running or scheduled. This lock serializes all scheduled bottom
 //        halves per GPU which service non-replayable faults.
-//      - gpu->isr.access_counters.service_lock:
+//      - gpu->parent->isr.access_counters.service_lock:
 //        Changes to the state of a GPU as it transitions from top-half to bottom-half
 //        interrupt handler for access counter notifications. This lock is acquired for
 //        that GPU, in the ISR top-half. Then a bottom-half is scheduled (to run in a
 //        workqueue). Then the bottom-half releases the lock when that GPU's processing
 //        appears to be done.
 //
-// - mmap_sem
-//      Order: UVM_LOCK_ORDER_MMAP_SEM
+// - mmap_lock (mmap_sem in kernels < 5.8)
+//      Order: UVM_LOCK_ORDER_MMAP_LOCK
 //      Reader/writer lock (rw_semaphore)
 //
-//      We're often called with the kernel already holding mmap_sem: mmap,
+//      We're often called with the kernel already holding mmap_lock: mmap,
 //      munmap, fault, etc. These operations may have to take any number of UVM
-//      locks, so mmap_sem requires special consideration in the lock order,
+//      locks, so mmap_lock requires special consideration in the lock order,
 //      since it's sometimes out of our control.
 //
-//      We need to hold mmap_sem when calling vm_insert_page, which means that
+//      We need to hold mmap_lock when calling vm_insert_page, which means that
 //      any time an operation (such as an ioctl) might need to install a CPU
-//      mapping, it must take current->mm->mmap_sem in read mode very early on.
+//      mapping, it must take current->mm->mmap_lock in read mode very early on.
 //
 //      However, current->mm is not necessarily the owning mm of the UVM vma.
 //      fork or fd passing via a UNIX doman socket can cause that. Notably, this
 //      is also the case when handling GPU faults from a kernel thread. This
-//      means we must lock current->mm->mmap_sem, then look up the UVM vma and
+//      means we must lock current->mm->mmap_lock, then look up the UVM vma and
 //      compare its mm before operating on that vma.
 //
-//      With HMM and ATS, the GPU fault handler takes mmap_sem. GPU faults may
+//      With HMM and ATS, the GPU fault handler takes mmap_lock. GPU faults may
 //      block forward progress of threads holding the RM GPUs lock until those
-//      faults are serviced, which means that mmap_sem cannot be held when the
-//      UVM driver calls into RM. In other words, mmap_sem and the RM GPUs lock
+//      faults are serviced, which means that mmap_lock cannot be held when the
+//      UVM driver calls into RM. In other words, mmap_lock and the RM GPUs lock
 //      are mutually exclusive.
 //
 // - Global VA spaces list lock
@@ -217,8 +217,8 @@
 //      Order: UVM_LOCK_ORDER_VA_SPACE
 //      Reader/writer lock (rw_semaphore) per uvm_va_space (UVM struct file)
 //
-//      This is the UVM equivalent of mmap_sem. It protects all state under that
-//      va_space, such as the VA range tree.
+//      This is the UVM equivalent of mmap_lock. It protects all state under 
+//      that va_space, such as the VA range tree.
 //
 //      Read mode: Faults (CPU and GPU), mapping creation, prefetches. These
 //      will be serialized at the VA block level if necessary. RM calls are
@@ -356,7 +356,7 @@ typedef enum
     UVM_LOCK_ORDER_GLOBAL_PM,
     UVM_LOCK_ORDER_GLOBAL,
     UVM_LOCK_ORDER_ISR,
-    UVM_LOCK_ORDER_MMAP_SEM,
+    UVM_LOCK_ORDER_MMAP_LOCK,
     UVM_LOCK_ORDER_VA_SPACES_LIST,
     UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS,
     UVM_LOCK_ORDER_VA_SPACE_READ_ACQUIRE_WRITE_RELEASE_LOCK,
@@ -450,29 +450,30 @@ bool __uvm_locking_initialized(void);
   // the given mode.
   #define uvm_check_locked(lock, flags) __uvm_check_locked((lock), (lock)->lock_order, (flags))
 
-  // Helpers for recording and asserting mmap_sem state
-  #define uvm_record_lock_mmap_sem_read(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
+  // Helpers for recording and asserting mmap_lock
+  // (mmap_sem in kernels < 5.8 ) state
+  #define uvm_record_lock_mmap_lock_read(mm) \
+          uvm_record_lock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
+  #define uvm_record_unlock_mmap_lock_read(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, \
+  #define uvm_record_unlock_mmap_lock_read_out_of_order(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, \
                                 UVM_LOCK_FLAGS_MODE_SHARED | UVM_LOCK_FLAGS_OUT_OF_ORDER)
 
-  #define uvm_record_lock_mmap_sem_write(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+  #define uvm_record_lock_mmap_lock_write(mm) \
+          uvm_record_lock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+  #define uvm_record_unlock_mmap_lock_write(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, \
+  #define uvm_record_unlock_mmap_lock_write_out_of_order(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, \
                                 UVM_LOCK_FLAGS_MODE_EXCLUSIVE | UVM_LOCK_FLAGS_OUT_OF_ORDER)
 
-  #define uvm_check_locked_mmap_sem(mmap_sem, flags) \
-           __uvm_check_locked((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, (flags))
+  #define uvm_check_locked_mmap_lock(mm, flags) \
+           __uvm_check_locked(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, (flags))
 
   // Helpers for recording RM API lock usage around UVM-RM interfaces
   #define uvm_record_lock_rm_api() \
@@ -505,14 +506,14 @@ bool __uvm_locking_initialized(void);
       return false;
   }
 
-  #define uvm_record_lock_mmap_sem_read                 UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read               UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read_out_of_order  UVM_IGNORE_EXPR
-  #define uvm_record_lock_mmap_sem_write                UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write              UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write_out_of_order UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_read                 UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read               UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read_out_of_order  UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_write                UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write              UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write_out_of_order UVM_IGNORE_EXPR
 
-  #define uvm_check_locked_mmap_sem                     uvm_check_locked
+  #define uvm_check_locked_mmap_lock                     uvm_check_locked
 
   #define uvm_record_lock_rm_api()
   #define uvm_record_unlock_rm_api()
@@ -529,47 +530,48 @@ bool __uvm_locking_initialized(void);
 #define uvm_assert_lockable_order(order) UVM_ASSERT(__uvm_check_lockable_order(order, UVM_LOCK_FLAGS_MODE_ANY))
 #define uvm_assert_unlocked_order(order) UVM_ASSERT(__uvm_check_unlocked_order(order))
 
-// Helpers for locking mmap_sem and recording its usage
-#define uvm_assert_mmap_sem_locked_mode(mmap_sem, flags) ({                          \
-      typeof(mmap_sem) _sem = (mmap_sem);                                            \
-      UVM_ASSERT(rwsem_is_locked(_sem) && uvm_check_locked_mmap_sem(_sem, (flags))); \
+// Helpers for locking mmap_lock (mmap_sem in kernels < 5.8)
+// and recording its usage
+#define uvm_assert_mmap_lock_locked_mode(mm, flags) ({                                      \
+      typeof(mm) _mm = (mm);                                                                \
+      UVM_ASSERT(nv_mm_rwsem_is_locked(_mm) && uvm_check_locked_mmap_lock((_mm), (flags))); \
   })
 
-#define uvm_assert_mmap_sem_locked(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_ANY)
-#define uvm_assert_mmap_sem_locked_read(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_SHARED)
-#define uvm_assert_mmap_sem_locked_write(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
-
-#define uvm_down_read_mmap_sem(mmap_sem) ({             \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_read(_sem);            \
-        down_read(_sem);                                \
+#define uvm_assert_mmap_lock_locked(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_FLAGS_MODE_ANY)
+#define uvm_assert_mmap_lock_locked_read(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_FLAGS_MODE_SHARED)
+#define uvm_assert_mmap_lock_locked_write(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+
+#define uvm_down_read_mmap_lock(mm) ({                  \
+        typeof(mm) _mm = (mm);                          \
+        uvm_record_lock_mmap_lock_read(_mm);            \
+        nv_mmap_read_lock(_mm);                         \
     })
 
-#define uvm_up_read_mmap_sem(mmap_sem) ({               \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_read(_sem);                                  \
-        uvm_record_unlock_mmap_sem_read(_sem);          \
+#define uvm_up_read_mmap_lock(mm) ({                    \
+        typeof(mm) _mm = (mm);                          \
+        nv_mmap_read_unlock(_mm);                       \
+        uvm_record_unlock_mmap_lock_read(_mm);          \
     })
 
-#define uvm_up_read_mmap_sem_out_of_order(mmap_sem) ({      \
-        typeof(mmap_sem) _sem = (mmap_sem);                 \
-        up_read(_sem);                                      \
-        uvm_record_unlock_mmap_sem_read_out_of_order(_sem); \
+#define uvm_up_read_mmap_lock_out_of_order(mm) ({           \
+        typeof(mm) _mm = (mm);                              \
+        nv_mmap_read_unlock(_mm);                           \
+        uvm_record_unlock_mmap_lock_read_out_of_order(_mm); \
     })
 
-#define uvm_down_write_mmap_sem(mmap_sem) ({            \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_write(_sem);           \
-        down_write(_sem);                               \
+#define uvm_down_write_mmap_lock(mm) ({                 \
+        typeof(mm) _mm = (mm);                          \
+        uvm_record_lock_mmap_lock_write(_mm);           \
+        nv_mmap_write_lock(_mm);                        \
     })
 
-#define uvm_up_write_mmap_sem(mmap_sem) ({              \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_write(_sem);                                 \
-        uvm_record_unlock_mmap_sem_write(_sem);         \
+#define uvm_up_write_mmap_lock(mm) ({                   \
+        typeof(mm) _mm = (mm);                          \
+        nv_mmap_write_unlock(_mm);                      \
+        uvm_record_unlock_mmap_lock_write(_mm);         \
     })
 
 // Helper for calling a UVM-RM interface function with lock recording
@@ -740,6 +742,20 @@ typedef struct
 
 #define uvm_assert_mutex_unlocked(uvm_mutex) UVM_ASSERT(!mutex_is_locked(&(uvm_mutex)->m))
 
+//
+// Linux kernel mutexes cannot be used with interrupts disabled. Doing so
+// can lead to deadlocks.
+// To warn about mutex usages with interrupts disabled, the following
+// macros and inline functions wrap around the raw kernel mutex operations
+// in order to check if the interrupts have been disabled and assert if so.
+//
+// TODO: Bug 2690258: evaluate whether !irqs_disabled() && !in_interrupt() is
+//       enough.
+//
+#define uvm_assert_mutex_interrupts() ({                                                                        \
+        UVM_ASSERT_MSG(!irqs_disabled() && !in_interrupt(), "Mutexes cannot be used with interrupts disabled"); \
+    })
+
 static void uvm_mutex_init(uvm_mutex_t *mutex, uvm_lock_order_t lock_order)
 {
     mutex_init(&mutex->m);
@@ -752,6 +768,7 @@ static void uvm_mutex_init(uvm_mutex_t *mutex, uvm_lock_order_t lock_order)
 
 #define uvm_mutex_lock(mutex) ({                                \
         typeof(mutex) _mutex = (mutex);                         \
+        uvm_assert_mutex_interrupts();                          \
         uvm_record_lock(_mutex, UVM_LOCK_FLAGS_MODE_EXCLUSIVE); \
         mutex_lock(&_mutex->m);                                 \
         uvm_assert_mutex_locked(_mutex);                        \
@@ -759,16 +776,21 @@ static void uvm_mutex_init(uvm_mutex_t *mutex, uvm_lock_order_t lock_order)
 
 // Lock w/o any tracking. This should be extremely rare and *_no_tracking
 // helpers will be added only as needed.
-#define uvm_mutex_lock_no_tracking(mutex) mutex_lock(&(mutex)->m)
+#define uvm_mutex_lock_no_tracking(mutex)  ({   \
+        uvm_assert_mutex_interrupts();          \
+        mutex_lock(&(mutex)->m);                \
+    })
 
 #define uvm_mutex_unlock(mutex) ({                                \
         typeof(mutex) _mutex = (mutex);                           \
+        uvm_assert_mutex_interrupts();                            \
         uvm_assert_mutex_locked(_mutex);                          \
         mutex_unlock(&_mutex->m);                                 \
         uvm_record_unlock(_mutex, UVM_LOCK_FLAGS_MODE_EXCLUSIVE); \
     })
 #define uvm_mutex_unlock_out_of_order(mutex) ({                                \
         typeof(mutex) _mutex = (mutex);                                        \
+        uvm_assert_mutex_interrupts();                                         \
         uvm_assert_mutex_locked(_mutex);                                       \
         mutex_unlock(&_mutex->m);                                              \
         uvm_record_unlock_out_of_order(_mutex, UVM_LOCK_FLAGS_MODE_EXCLUSIVE); \
@@ -776,7 +798,10 @@ static void uvm_mutex_init(uvm_mutex_t *mutex, uvm_lock_order_t lock_order)
 
 // Unlock w/o any tracking. This should be extremely rare and *_no_tracking
 // helpers will be added only as needed.
-#define uvm_mutex_unlock_no_tracking(mutex) mutex_unlock(&(mutex)->m)
+#define uvm_mutex_unlock_no_tracking(mutex) ({  \
+        uvm_assert_mutex_interrupts();          \
+        mutex_unlock(&(mutex)->m);              \
+    })
 
 typedef struct
 {
@@ -795,6 +820,8 @@ static void uvm_sema_init(uvm_semaphore_t *semaphore, int val, uvm_lock_order_t
 #endif
 }
 
+#define uvm_sem_is_locked(uvm_sem) uvm_check_locked(uvm_sem, UVM_LOCK_FLAGS_MODE_SHARED)
+
 #define uvm_down(uvm_sem) ({                               \
         typeof(uvm_sem) _sem = (uvm_sem);                  \
         uvm_record_lock(_sem, UVM_LOCK_FLAGS_MODE_SHARED); \
@@ -803,15 +830,18 @@ static void uvm_sema_init(uvm_semaphore_t *semaphore, int val, uvm_lock_order_t
 
 #define uvm_up(uvm_sem) ({                                   \
         typeof(uvm_sem) _sem = (uvm_sem);                    \
+        UVM_ASSERT(uvm_sem_is_locked(_sem));                 \
         up(&_sem->sem);                                      \
         uvm_record_unlock(_sem, UVM_LOCK_FLAGS_MODE_SHARED); \
     })
 #define uvm_up_out_of_order(uvm_sem) ({                                   \
         typeof(uvm_sem) _sem = (uvm_sem);                                 \
+        UVM_ASSERT(uvm_sem_is_locked(_sem));                              \
         up(&_sem->sem);                                                   \
         uvm_record_unlock_out_of_order(_sem, UVM_LOCK_FLAGS_MODE_SHARED); \
     })
 
+
 // A regular spinlock
 // Locked/unlocked with uvm_spin_lock()/uvm_spin_unlock()
 typedef struct
diff --git a/nvidia-uvm/uvm8_lock_test.c b/nvidia-uvm/uvm8_lock_test.c
index 583f0db..926b593 100644
--- a/nvidia-uvm/uvm8_lock_test.c
+++ b/nvidia-uvm/uvm8_lock_test.c
@@ -64,7 +64,7 @@ static bool fake_check_locked(uvm_lock_order_t lock_order, uvm_lock_flags_t flag
 
 // TODO: Bug 1799173: The lock asserts verify that the RM GPU lock isn't taken
 //       with the VA space lock in exclusive mode, and that the RM GPU lock
-//       isn't taken with mmap_sem held in any mode. Hack around this in the
+//       isn't taken with mmap_lock held in any mode. Hack around this in the
 //       test to enable the checks until we figure out something better.
 static bool skip_lock(uvm_lock_order_t lock_order, uvm_lock_flags_t flags)
 {
@@ -73,7 +73,7 @@ static bool skip_lock(uvm_lock_order_t lock_order, uvm_lock_flags_t flags)
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS)
         return mode_flags == UVM_LOCK_FLAGS_MODE_EXCLUSIVE;
 
-    return lock_order == UVM_LOCK_ORDER_MMAP_SEM;
+    return lock_order == UVM_LOCK_ORDER_MMAP_LOCK;
 }
 
 static NV_STATUS test_all_locks_from(uvm_lock_order_t from_lock_order)
diff --git a/nvidia-uvm/uvm8_mem.c b/nvidia-uvm/uvm8_mem.c
index c0eed57..5c74875 100644
--- a/nvidia-uvm/uvm8_mem.c
+++ b/nvidia-uvm/uvm8_mem.c
@@ -481,7 +481,7 @@ static NV_STATUS uvm_mem_map_cpu_to_sysmem_user(uvm_mem_t *mem, struct vm_area_s
 
     UVM_ASSERT(uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
 
     // TODO: Bug 1995015: high-order page allocations need to be allocated as
     // compound pages in order to be able to use vm_insert_page on them. This
@@ -505,7 +505,7 @@ static NV_STATUS uvm_mem_map_cpu_to_vidmem_user(uvm_mem_t *mem, struct vm_area_s
     size_t num_chunk_pages = mem->chunk_size / PAGE_SIZE;
 
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     UVM_ASSERT(!uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->backing_gpu != NULL);
     UVM_ASSERT(mem->backing_gpu->numa_info.enabled);
diff --git a/nvidia-uvm/uvm8_migrate.c b/nvidia-uvm/uvm8_migrate.c
index 0c72438..455e5b7 100644
--- a/nvidia-uvm/uvm8_migrate.c
+++ b/nvidia-uvm/uvm8_migrate.c
@@ -596,7 +596,7 @@ static NV_STATUS uvm_migrate(uvm_va_space_t *va_space,
     bool is_single_block;
     bool should_do_cpu_preunmap;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(current->mm);
     uvm_assert_rwsem_locked(&va_space->lock);
 
     if (!first_va_range || first_va_range->type != UVM_VA_RANGE_TYPE_MANAGED)
@@ -855,7 +855,7 @@ NV_STATUS uvm_api_migrate(UVM_MIGRATE_PARAMS *params, struct file *filp)
     }
 
     // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (!is_async) {
@@ -937,7 +937,7 @@ done:
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(current->mm);
 
     if (tracker_ptr) {
         if (params->semaphoreAddress && status == NV_OK) {
@@ -983,7 +983,7 @@ NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, st
     uvm_gpu_t *gpu = NULL;
 
     // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->destinationUuid)) {
@@ -1028,7 +1028,7 @@ done:
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(current->mm);
 
     tracker_status = uvm_tracker_wait_deinit(&local_tracker);
     uvm_va_space_up_read(va_space);
diff --git a/nvidia-uvm/uvm8_policy.c b/nvidia-uvm/uvm8_policy.c
index c27df95..d4be3c9 100644
--- a/nvidia-uvm/uvm8_policy.c
+++ b/nvidia-uvm/uvm8_policy.c
@@ -39,7 +39,7 @@ bool uvm_is_valid_vma_range(NvU64 start, NvU64 length)
     const NvU64 end = start + length;
     struct vm_area_struct *vma;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(current->mm);
 
     vma = find_vma_intersection(current->mm, start, end);
 
@@ -234,7 +234,7 @@ NV_STATUS uvm_api_set_preferred_location(const UVM_SET_PREFERRED_LOCATION_PARAMS
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
     has_va_space_write_lock = true;
 
@@ -307,7 +307,7 @@ done:
         uvm_va_space_up_write(va_space);
     else
         uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status == NV_OK ? tracker_status : status;
 }
@@ -319,7 +319,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, params->requestedBase, params->length);
@@ -330,7 +330,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
         status = NV_OK;
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
@@ -411,7 +411,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -475,7 +475,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status;
 }
@@ -658,7 +658,7 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
     UVM_ASSERT(va_space);
 
     // We need mmap_sem as we may create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -712,7 +712,8 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
+
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_populate_pageable.c b/nvidia-uvm/uvm8_populate_pageable.c
index c47975e..10e5a71 100644
--- a/nvidia-uvm/uvm8_populate_pageable.c
+++ b/nvidia-uvm/uvm8_populate_pageable.c
@@ -48,7 +48,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     UVM_ASSERT(PAGE_ALIGNED(outer));
     UVM_ASSERT(vma->vm_end > start);
     UVM_ASSERT(vma->vm_start < outer);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     if (!min_prot_ok)
         return NV_ERR_INVALID_ADDRESS;
@@ -66,12 +66,12 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     // page fault handler. The page fault is caused by get_user_pages.
     uvm_managed_vma = uvm_file_is_nvidia_uvm(vma->vm_file);
     if (uvm_managed_vma)
-        uvm_record_unlock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_read(mm);
 
     ret = NV_GET_USER_PAGES(start, vma_num_pages, is_writable, 0, NULL, NULL);
 
     if (uvm_managed_vma)
-        uvm_record_lock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_lock_mmap_lock_read(mm);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
@@ -94,7 +94,7 @@ NV_STATUS uvm_populate_pageable(struct mm_struct *mm,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     vma = find_vma_intersection(mm, start, outer);
 
@@ -155,14 +155,14 @@ NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params,
 
     // mmap_sem is needed to traverse the vmas in the input range and call into
     // get_user_pages
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
 
     if (allow_managed || uvm_va_space_range_empty(va_space, params->base, params->base + params->length - 1))
         status = uvm_populate_pageable(current->mm, params->base, params->length, min_prot);
     else
         status = NV_ERR_INVALID_ADDRESS;
 
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status;
 }
diff --git a/nvidia-uvm/uvm8_tools.c b/nvidia-uvm/uvm8_tools.c
index 0415d26..1300280 100644
--- a/nvidia-uvm/uvm8_tools.c
+++ b/nvidia-uvm/uvm8_tools.c
@@ -264,9 +264,9 @@ static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct p
         goto fail;
     }
 
-    down_read(&current->mm->mmap_sem);
+    nv_mmap_read_lock(current->mm);
     ret = NV_GET_USER_PAGES(user_va, num_pages, 1, 0, *pages, vmas);
-    up_read(&current->mm->mmap_sem);
+    nv_mmap_read_unlock(current->mm);
     if (ret != num_pages) {
         status = NV_ERR_INVALID_ARGUMENT;
         goto fail;
diff --git a/nvidia-uvm/uvm8_va_block.c b/nvidia-uvm/uvm8_va_block.c
index 45ba210..c64f5a5 100644
--- a/nvidia-uvm/uvm8_va_block.c
+++ b/nvidia-uvm/uvm8_va_block.c
@@ -6516,7 +6516,7 @@ static NV_STATUS block_map_cpu_page_to(uvm_va_block_t *block,
     // us, so we can safely operate on the vma but we can't use
     // uvm_va_range_vma_current.
     vma = uvm_va_range_vma(va_range);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
 
     // Add the mapping
     addr = uvm_va_block_cpu_page_address(block, page_index);
@@ -10662,7 +10662,7 @@ NV_STATUS uvm8_test_change_pte_mapping(UVM_TEST_CHANGE_PTE_MAPPING_PARAMS *param
 
     // mmap_sem isn't needed for invalidating CPU mappings, but it will be
     // needed for inserting them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->uuid)) {
@@ -10745,7 +10745,7 @@ out_block:
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_va_block_context_free(block_context);
 
@@ -10760,7 +10760,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
     BUILD_BUG_ON(UVM_TEST_VA_BLOCK_SIZE != UVM_VA_BLOCK_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     status = uvm_va_block_find(va_space, params->lookup_address, &va_block);
@@ -10778,7 +10778,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
@@ -10795,7 +10795,7 @@ NV_STATUS uvm8_test_va_residency_info(UVM_TEST_VA_RESIDENCY_INFO_PARAMS *params,
     unsigned release_block_count = 0;
     NvU64 addr = UVM_ALIGN_DOWN(params->lookup_address, PAGE_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, addr);
@@ -10938,7 +10938,7 @@ out:
             uvm_va_block_release(block);
     }
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.c b/nvidia-uvm/uvm8_va_range.c
index c390027..d4e37d8 100644
--- a/nvidia-uvm/uvm8_va_range.c
+++ b/nvidia-uvm/uvm8_va_range.c
@@ -1804,7 +1804,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
     va_space = uvm_va_space_get(filp);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, params->lookup_address);
@@ -1865,7 +1865,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.h b/nvidia-uvm/uvm8_va_range.h
index d7a8c3a..77ddcbf 100644
--- a/nvidia-uvm/uvm8_va_range.h
+++ b/nvidia-uvm/uvm8_va_range.h
@@ -718,7 +718,7 @@ static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, s
     if (mm != vma->vm_mm)
         return NULL;
 
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     return vma;
 }
 
diff --git a/nvidia-uvm/uvm8_va_space.c b/nvidia-uvm/uvm8_va_space.c
index f631c99..0e17915 100644
--- a/nvidia-uvm/uvm8_va_space.c
+++ b/nvidia-uvm/uvm8_va_space.c
@@ -516,7 +516,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
     if (flags & ~UVM_INIT_FLAGS_MASK)
         return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_write_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     if (atomic_read(&va_space->initialized)) {
@@ -545,7 +545,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_write_mmap_lock(current->mm);
 
     return status;
 }
@@ -833,7 +833,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
 
     // The mmap_sem lock is needed to establish CPU mappings to any pages
     // evicted from the GPU if accessed by CPU is set for them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -850,7 +850,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
     uvm_processor_mask_clear(&va_space->gpu_unregister_in_progress, gpu->id);
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_deferred_free_object_list(&deferred_free_list);
 
@@ -1276,9 +1276,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     // need mmap_sem in read mode to handle potential CPU mapping changes in
     // uvm_va_range_add_gpu_va_space().
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_write_mmap_lock(current->mm);
     else
-        uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_read_mmap_lock(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -1333,9 +1333,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(current->mm);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(current->mm);
 
     uvm_gpu_release(gpu);
     return NV_OK;
@@ -1354,9 +1354,9 @@ error:
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(current->mm);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(current->mm);
 
     destroy_gpu_va_space(gpu_va_space);
 
@@ -1430,7 +1430,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     uvm_gpu_retain(gpu);
     uvm_va_space_up_read_rm(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     // We dropped the lock so we have to re-verify that this gpu_va_space is
@@ -1448,7 +1448,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     }
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_deferred_free_object_list(&deferred_free_list);
     uvm_gpu_va_space_release(gpu_va_space);
diff --git a/nvidia-uvm/uvm8_va_space_mm.c b/nvidia-uvm/uvm8_va_space_mm.c
index fb029b3..b816c99 100644
--- a/nvidia-uvm/uvm8_va_space_mm.c
+++ b/nvidia-uvm/uvm8_va_space_mm.c
@@ -240,7 +240,7 @@ bool uvm_va_space_mm_enabled(uvm_va_space_t *va_space)
     static int uvm_mmu_notifier_register(uvm_va_space_mm_t *va_space_mm)
     {
         UVM_ASSERT(va_space_mm->mm);
-        uvm_assert_mmap_sem_locked_write(&va_space_mm->mm->mmap_sem);
+        uvm_assert_mmap_lock_locked_write(va_space_mm->mm);
 
         if (UVM_ATS_IBM_SUPPORTED_IN_DRIVER() && g_uvm_global.ats.enabled)
             va_space_mm->mmu_notifier.ops = &uvm_mmu_notifier_ops_ats;
@@ -272,7 +272,7 @@ NV_STATUS uvm_va_space_mm_register(uvm_va_space_t *va_space)
     uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
     int ret;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(current->mm);
     uvm_assert_rwsem_locked_write(&va_space->lock);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) != NV_OK);
@@ -307,7 +307,7 @@ void uvm_va_space_mm_unregister(uvm_va_space_t *va_space)
     // mmu_notifier_unregister() may trigger uvm_va_space_mm_shutdown(), which
     // takes those locks and also waits for other threads which may take those
     // locks.
-    uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_SEM);
+    uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_LOCK);
     uvm_assert_unlocked_order(UVM_LOCK_ORDER_VA_SPACE);
 
     if (!va_space_mm->mm)
@@ -540,9 +540,9 @@ static NV_STATUS mm_read64(struct mm_struct *mm, NvU64 addr, NvU64 *val)
 
     UVM_ASSERT(IS_ALIGNED(addr, sizeof(val)));
 
-    uvm_down_read_mmap_sem(&mm->mmap_sem);
+    uvm_down_read_mmap_lock(mm);
     ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)addr, 1, write, force, &page, NULL);
-    uvm_up_read_mmap_sem(&mm->mmap_sem);
+    uvm_up_read_mmap_lock(mm);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
diff --git a/nvidia/linux_nvswitch.c b/nvidia/linux_nvswitch.c
index 6d13529..e35a8d5 100644
--- a/nvidia/linux_nvswitch.c
+++ b/nvidia/linux_nvswitch.c
@@ -26,6 +26,7 @@
 #include "conftest.h"
 #include "nvmisc.h"
 #include "nv-linux.h"
+#include "nv-time.h"
 #include "nv-procfs.h"
 #include "nvlink_common.h"
 #include "nvlink_errors.h"
diff --git a/nvidia/nvidia.Kbuild b/nvidia/nvidia.Kbuild
index ddc548d..cffb4f5 100644
--- a/nvidia/nvidia.Kbuild
+++ b/nvidia/nvidia.Kbuild
@@ -171,7 +171,9 @@ NV_CONFTEST_TYPE_COMPILE_TESTS += node_states_n_memory
 NV_CONFTEST_TYPE_COMPILE_TESTS += kmem_cache_has_kobj_remove_work
 NV_CONFTEST_TYPE_COMPILE_TESTS += sysfs_slab_unlink
 NV_CONFTEST_TYPE_COMPILE_TESTS += proc_ops
+NV_CONFTEST_TYPE_COMPILE_TESTS += vmalloc_has_pgprot_t_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += timeval
+NV_CONFTEST_TYPE_COMPILE_TESTS += mm_has_mmap_lock
 
 NV_CONFTEST_GENERIC_COMPILE_TESTS += dom0_kernel_present
 NV_CONFTEST_GENERIC_COMPILE_TESTS += nvidia_vgpu_hyperv_available
diff --git a/nvidia/nvlink_linux.c b/nvidia/nvlink_linux.c
index bb5b026..7d067d7 100644
--- a/nvidia/nvlink_linux.c
+++ b/nvidia/nvlink_linux.c
@@ -276,7 +276,6 @@ static long nvlink_fops_unlocked_ioctl(struct file *file,
     return nvlink_fops_ioctl(NV_FILE_INODE(file), file, cmd, arg);
 }
 
-
 static const struct file_operations nvlink_fops = {
     .owner           = THIS_MODULE,
     .open            = nvlink_fops_open,
diff --git a/nvidia/os-mlock.c b/nvidia/os-mlock.c
index 1900fa1..237e2ec 100644
--- a/nvidia/os-mlock.c
+++ b/nvidia/os-mlock.c
@@ -43,7 +43,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
 
     vma = find_vma(mm, (NvUPtr)address);
     if ((vma == NULL) || ((vma->vm_flags & (VM_IO | VM_PFNMAP)) == 0))
@@ -76,7 +76,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
     }
 
 done:
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
 
     return rmStatus;
 }
@@ -110,10 +110,10 @@ NV_STATUS NV_API_CALL os_lock_user_pages(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
     ret = NV_GET_USER_PAGES((unsigned long)address,
                             page_count, write, force, user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
     pinned = ret;
 
     if (ret < 0)
-- 
2.25.1

