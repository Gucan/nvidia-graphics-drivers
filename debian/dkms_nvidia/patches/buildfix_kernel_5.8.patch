From 84f61c0f7fc705794201d0094cc6306735e13afc Mon Sep 17 00:00:00 2001
From: Alberto Milone <alberto.milone@canonical.com>
Date: Mon, 27 Jul 2020 12:29:48 +0200
Subject: [PATCH 1/1] Add support for Linux 5.8

V2

Signed-off-by: Alberto Milone <alberto.milone@canonical.com>
---
 common/inc/nv-linux.h                       |   4 +
 common/inc/nv-mm.h                          |  54 ++++++++
 common/inc/nv-procfs.h                      |  54 +++++++-
 conftest.sh                                 |  88 +++++++++++++
 nvidia-drm/nvidia-drm-linux.c               |   4 +-
 nvidia-drm/nvidia-drm.Kbuild                |   1 +
 nvidia-uvm/uvm8.c                           |  35 ++---
 nvidia-uvm/uvm8_ats_faults.h                |   2 +-
 nvidia-uvm/uvm8_ats_ibm.c                   |   4 +-
 nvidia-uvm/uvm8_gpu_access_counters.c       |   4 +-
 nvidia-uvm/uvm8_gpu_non_replayable_faults.c |   4 +-
 nvidia-uvm/uvm8_gpu_replayable_faults.c     |   6 +-
 nvidia-uvm/uvm8_hmm.c                       |   2 +-
 nvidia-uvm/uvm8_lock.c                      |   6 +-
 nvidia-uvm/uvm8_lock.h                      | 134 +++++++++++---------
 nvidia-uvm/uvm8_lock_test.c                 |   4 +-
 nvidia-uvm/uvm8_mem.c                       |   4 +-
 nvidia-uvm/uvm8_migrate.c                   |  10 +-
 nvidia-uvm/uvm8_migrate_pageable.c          |   8 +-
 nvidia-uvm/uvm8_policy.c                    |  19 +--
 nvidia-uvm/uvm8_populate_pageable.c         |  12 +-
 nvidia-uvm/uvm8_tools.c                     |   4 +-
 nvidia-uvm/uvm8_va_block.c                  |  14 +-
 nvidia-uvm/uvm8_va_range.c                  |   4 +-
 nvidia-uvm/uvm8_va_range.h                  |   2 +-
 nvidia-uvm/uvm8_va_space.c                  |  24 ++--
 nvidia-uvm/uvm8_va_space_mm.c               |  10 +-
 nvidia/nvidia.Kbuild                        |   2 +
 nvidia/nvlink_linux.c                       |   1 -
 nvidia/os-mlock.c                           |   8 +-
 30 files changed, 374 insertions(+), 154 deletions(-)

diff --git a/common/inc/nv-linux.h b/common/inc/nv-linux.h
index 2dc8e26..fb6597a 100644
--- a/common/inc/nv-linux.h
+++ b/common/inc/nv-linux.h
@@ -505,7 +505,11 @@ extern NvBool nvos_is_chipset_io_coherent(void);
 
 static inline void *nv_vmalloc(unsigned long size)
 {
+#if defined(NV_VMALLOC_HAS_PGPROT_T_ARG)
     void *ptr = __vmalloc(size, GFP_KERNEL, PAGE_KERNEL);
+#else
+    void *ptr = __vmalloc(size, GFP_KERNEL);
+#endif
     if (ptr)
         NV_MEMDBG_ADD(ptr, size);
     return ptr;
diff --git a/common/inc/nv-mm.h b/common/inc/nv-mm.h
index d90f899..b04ff67 100644
--- a/common/inc/nv-mm.h
+++ b/common/inc/nv-mm.h
@@ -201,4 +201,58 @@ static inline unsigned long nv_page_fault_va(struct vm_fault *vmf)
 #endif
 }
 
+static inline void nv_mmap_read_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_read_lock(mm);
+#else
+    down_read(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_read_unlock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_read_unlock(mm);
+#else
+    up_read(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_write_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_write_lock(mm);
+#else
+    down_write(&mm->mmap_sem);
+#endif
+}
+
+static inline void nv_mmap_write_unlock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    mmap_write_unlock(mm);
+#else
+    up_write(&mm->mmap_sem);
+#endif
+}
+
+static inline int nv_mm_rwsem_is_locked(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    return rwsem_is_locked(&mm->mmap_lock);
+#else
+    return rwsem_is_locked(&mm->mmap_sem);
+#endif
+}
+
+static inline struct rw_semaphore *nv_mmap_get_lock(struct mm_struct *mm)
+{
+#if defined(NV_MM_HAS_MMAP_LOCK)
+    return &mm->mmap_lock;
+#else
+    return &mm->mmap_sem;
+#endif
+}
+
 #endif // __NV_MM_H__
diff --git a/common/inc/nv-procfs.h b/common/inc/nv-procfs.h
index 107b324..9065ada 100644
--- a/common/inc/nv-procfs.h
+++ b/common/inc/nv-procfs.h
@@ -50,6 +50,18 @@ typedef struct file_operations nv_proc_ops_t;
 #define NV_PROC_OPS_RELEASE release
 #endif
 
+#if defined(NV_PROCFS_PROC_OPS_PRESENT)
+#define NV_CREATE_PROC_FILE(filename,parent,__name,__data)               \
+   ({                                                                    \
+        struct proc_dir_entry *__entry;                                  \
+        int mode = (S_IFREG | S_IRUGO);                                  \
+        const struct proc_ops *fops = &nv_procfs_##__name##_fops;		 \
+        if (fops->proc_write != 0)                                            \
+            mode |= S_IWUSR;                                             \
+        __entry = proc_create_data(filename, mode, parent, fops, __data);\
+        __entry;                                                         \
+    })
+#else
 #define NV_CREATE_PROC_FILE(filename,parent,__name,__data)               \
    ({                                                                    \
         struct proc_dir_entry *__entry;                                  \
@@ -60,6 +72,7 @@ typedef struct file_operations nv_proc_ops_t;
         __entry = proc_create_data(filename, mode, parent, fops, __data);\
         __entry;                                                         \
     })
+#endif
 
 /*
  * proc_mkdir_mode exists in Linux 2.6.9, but isn't exported until Linux 3.0.
@@ -102,6 +115,45 @@ typedef struct file_operations nv_proc_ops_t;
 void nv_procfs_unregister_all(struct proc_dir_entry *entry,
                               struct proc_dir_entry *delimiter);
 
+#if defined(NV_PROCFS_PROC_OPS_PRESENT)
+#define NV_DEFINE_SINGLE_PROCFS_FILE(name, open_callback, close_callback)     \
+    static int nv_procfs_open_##name(                                         \
+        struct inode *inode,                                                  \
+        struct file *filep                                                    \
+    )                                                                         \
+    {                                                                         \
+        int ret;                                                              \
+        ret = single_open(filep, nv_procfs_read_##name,                       \
+                          NV_PDE_DATA(inode));                                \
+        if (ret < 0)                                                          \
+        {                                                                     \
+            return ret;                                                       \
+        }                                                                     \
+        ret = open_callback();                                                \
+        if (ret < 0)                                                          \
+        {                                                                     \
+            single_release(inode, filep);                                     \
+        }                                                                     \
+        return ret;                                                           \
+    }                                                                         \
+                                                                              \
+    static int nv_procfs_release_##name(                                      \
+        struct inode *inode,                                                  \
+        struct file *filep                                                    \
+    )                                                                         \
+    {                                                                         \
+        close_callback();                                                     \
+        return single_release(inode, filep);                                  \
+    }                                                                         \
+                                                                              \
+    static const struct proc_ops nv_procfs_##name##_fops = {           \
+        .proc_open       = nv_procfs_open_##name,                                  \
+        .proc_read       = seq_read,                                               \
+        .proc_lseek     = seq_lseek,                                              \
+        .proc_release    = nv_procfs_release_##name,                               \
+    };
+
+#else
 #define NV_DEFINE_SINGLE_PROCFS_FILE(name, open_callback, close_callback)     \
     static int nv_procfs_open_##name(                                         \
         struct inode *inode,                                                  \
@@ -139,7 +191,7 @@ void nv_procfs_unregister_all(struct proc_dir_entry *entry,
         .NV_PROC_OPS_LSEEK   = seq_lseek,                                     \
         .NV_PROC_OPS_RELEASE = nv_procfs_release_##name,                      \
     };
-
+#endif
 #endif  /* CONFIG_PROC_FS */
 
 #endif /* _NV_PROCFS_H */
diff --git a/conftest.sh b/conftest.sh
index a3b4481..0b97ff2 100755
--- a/conftest.sh
+++ b/conftest.sh
@@ -1347,6 +1347,40 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_PROC_REMOVE_PRESENT" "" "functions"
         ;;
 
+        proc_create)
+            #
+            # Determine if the proc_*() function rely on file_operations.
+            #
+            # Added by commit a8ca16ea7b0a ("proc: Supply a function to
+            # Replaced by commit 97a32539b956 ("proc: convert everything
+            # to "struct proc_ops"
+            #
+            CODE="
+            #include <linux/proc_fs.h>
+            #include <linux/seq_file.h>
+
+            static int conftest_proc_show(struct seq_file *m, void *v) {
+                return 0;
+            }
+
+            static int conftest_proc_open(struct inode *inode, struct  file *file) {
+              return single_open(file, conftest_proc_show, NULL);
+            }
+
+            static const struct proc_ops conftest_proc_ops = {
+                .proc_open = conftest_proc_open,
+                .proc_read = seq_read,
+                .proc_lseek = seq_lseek,
+                .proc_release = single_release,
+            };
+
+            void conftest_proc_create(void) {
+                proc_create(\"conftest_proc\", 0, NULL, &conftest_proc_ops);
+            }"
+
+            compile_check_conftest "$CODE" "NV_PROCFS_PROC_OPS_PRESENT" "" "types"
+        ;;
+
         backing_dev_info)
             #
             # Determine if the 'address_space' structure has
@@ -2997,6 +3031,22 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_TIMER_SETUP_PRESENT" "" "functions"
         ;;
 
+        timeval_structs)
+            #
+            # Determine if timespec and timeval structs are present.
+            #
+            # Hidden by commit c766d1472c70 ("y2038: hide timeval/timespec/itimerval/
+            # itimerspec types") (2020-02-20)
+            #
+            CODE="
+            #include <linux/time.h>
+            void conftest_timeval_structs(void) {
+                struct timeval tmp;
+            }"
+            compile_check_conftest "$CODE" "NV_TIME_STRUCTS_PRESENT" "" "types"
+        ;;
+
+
         radix_tree_replace_slot)
             #
             # Determine if the radix_tree_replace_slot() function is
@@ -3763,6 +3813,44 @@ compile_test() {
             compile_check_conftest "$CODE" "NV_KTIME_GET_REAL_TS64_PRESENT" "" "functions"
         ;;
 
+        vmalloc_has_pgprot_t_arg)
+            #
+            # Determine if __vmalloc has the 'pgprot' argument.
+            #
+            # The third argument to __vmalloc, page protection
+            # 'pgprot_t prot', was removed by commit 88dca4ca5a93
+            # (mm: remove the pgprot argument to __vmalloc)
+            # in v5.8-rc1 (2020-06-01).
+        CODE="
+        #include <linux/vmalloc.h>
+
+        void conftest_vmalloc_has_pgprot_t_arg(void) {
+            pgprot_t prot;
+            (void)__vmalloc(0, 0, prot);
+        }"
+
+            compile_check_conftest "$CODE" "NV_VMALLOC_HAS_PGPROT_T_ARG" "" "types"
+
+        ;;
+
+        mm_has_mmap_lock)
+            #
+            # Determine if the 'mm_struct' structure has a 'mmap_lock' field.
+            #
+            # Kernel commit da1c55f1b272 ("mmap locking API: rename mmap_sem
+            # to mmap_lock") replaced the field 'mmap_sem' by 'mmap_lock'
+            # in v5.8-rc1 (2020-06-08).
+            CODE="
+            #include <linux/mm_types.h>
+
+            int conftest_mm_has_mmap_lock(void) {
+                return offsetof(struct mm_struct, mmap_lock);
+            }"
+
+            compile_check_conftest "$CODE" "NV_MM_HAS_MMAP_LOCK" "" "types"
+
+        ;;
+
         # When adding a new conftest entry, please use the correct format for
         # specifying the relevant upstream Linux kernel commit.
         #
diff --git a/nvidia-drm/nvidia-drm-linux.c b/nvidia-drm/nvidia-drm-linux.c
index 1d3e658..f8fcde4 100644
--- a/nvidia-drm/nvidia-drm-linux.c
+++ b/nvidia-drm/nvidia-drm-linux.c
@@ -103,11 +103,11 @@ int nv_drm_lock_user_pages(unsigned long address,
         return -ENOMEM;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
 
     pages_pinned = NV_GET_USER_PAGES(address, pages_count, write, force,
                                      user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
 
     if (pages_pinned < 0 || (unsigned)pages_pinned < pages_count) {
         goto failed;
diff --git a/nvidia-drm/nvidia-drm.Kbuild b/nvidia-drm/nvidia-drm.Kbuild
index 09d5ca4..fd17ea1 100644
--- a/nvidia-drm/nvidia-drm.Kbuild
+++ b/nvidia-drm/nvidia-drm.Kbuild
@@ -92,3 +92,4 @@ NV_CONFTEST_TYPE_COMPILE_TESTS += drm_atomic_helper_swap_state_has_stall_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_driver_prime_flag_present
 NV_CONFTEST_TYPE_COMPILE_TESTS += vm_fault_t
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_gem_object_has_resv
+NV_CONFTEST_TYPE_COMPILE_TESTS += mm_has_mmap_lock
diff --git a/nvidia-uvm/uvm8.c b/nvidia-uvm/uvm8.c
index 447f72f..14289f8 100644
--- a/nvidia-uvm/uvm8.c
+++ b/nvidia-uvm/uvm8.c
@@ -367,8 +367,9 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
         return;
     }
 
-    // At this point we are guaranteed that the mmap_sem is held in write mode.
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    // At this point we are guaranteed that the mmap_lock is held in write
+    // mode.
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     // Split vmas should always fall entirely within the old one, and be on one
     // side.
@@ -417,7 +418,7 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_open_managed_entry(struct vm_area_struct *vma)
@@ -432,7 +433,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     bool make_zombie = false;
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(current->mm);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) == NV_OK);
 
@@ -460,7 +461,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
         }
     }
 
-    // See uvm_mmap for why we need this in addition to mmap_sem
+    // See uvm_mmap for why we need this in addition to mmap_lock
     uvm_va_space_down_write(va_space);
 
     uvm_destroy_vma_managed(vma, make_zombie);
@@ -476,7 +477,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     uvm_va_space_up_write(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_close_managed_entry(struct vm_area_struct *vma)
@@ -521,10 +522,10 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 
     service_context->cpu_fault.wakeup_time_stamp = 0;
 
-    // The mmap_sem might be held in write mode, but the mode doesn't matter for
-    // the purpose of lock ordering and we don't rely on it being in write
+    // The mmap_lock might be held in write mode, but the mode doesn't matter
+    // for the purpose of lock ordering and we don't rely on it being in write
     // anywhere so just record it as read mode in all cases.
-    uvm_record_lock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_lock_mmap_lock_read(vma->vm_mm);
 
     do {
         bool do_sleep = false;
@@ -587,7 +588,7 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
     }
 
     uvm_va_space_up_read(va_space);
-    uvm_record_unlock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_read(vma->vm_mm);
 
     if (status == NV_OK) {
         status = uvm_global_mask_check_ecc_error(&gpus_to_check_for_ecc);
@@ -663,7 +664,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
     bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
     NV_STATUS status;
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -703,7 +704,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
 
     uvm_va_space_up_write(va_space);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_open_semaphore_pool_entry(struct vm_area_struct *vma)
@@ -718,7 +719,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(current->mm);
 
     uvm_va_space_down_read(va_space);
 
@@ -727,7 +728,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_up_read(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
 static void uvm_vm_close_semaphore_pool_entry(struct vm_area_struct *vma)
@@ -797,7 +798,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
         return 0;
     }
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(current->mm);
 
     // VM_MIXEDMAP      Required to use vm_insert_page
     //
@@ -823,7 +824,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
     }
     vma_wrapper_allocated = true;
 
-    // The kernel has taken mmap_sem in write mode, but that doesn't prevent
+    // The kernel has taken mmap_lock in write mode, but that doesn't prevent
     // this va_space from being modified by the GPU fault path or from the ioctl
     // path where we don't have this mm for sure, so we have to lock the VA
     // space directly.
@@ -864,7 +865,7 @@ out:
     if (ret != 0 && vma_wrapper_allocated)
         uvm_vma_wrapper_destroy(vma->vm_private_data);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(current->mm);
 
     uvm_up_read(&g_uvm_global.pm.lock);
 
diff --git a/nvidia-uvm/uvm8_ats_faults.h b/nvidia-uvm/uvm8_ats_faults.h
index e001b60..1d10534 100644
--- a/nvidia-uvm/uvm8_ats_faults.h
+++ b/nvidia-uvm/uvm8_ats_faults.h
@@ -39,7 +39,7 @@ NV_STATUS uvm_ats_invalidate_tlbs(uvm_gpu_va_space_t *gpu_va_space,
 static bool uvm_can_ats_service_faults(uvm_gpu_va_space_t *gpu_va_space, struct mm_struct *mm)
 {
     if (mm)
-        uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+        uvm_assert_mmap_lock_locked(mm);
     if (gpu_va_space->ats.enabled)
         UVM_ASSERT(g_uvm_global.ats.enabled);
 
diff --git a/nvidia-uvm/uvm8_ats_ibm.c b/nvidia-uvm/uvm8_ats_ibm.c
index 93ede03..f94e934 100644
--- a/nvidia-uvm/uvm8_ats_ibm.c
+++ b/nvidia-uvm/uvm8_ats_ibm.c
@@ -275,7 +275,7 @@ static NV_STATUS uvm_ats_ibm_register_gpu_va_space_kernel(uvm_gpu_va_space_t *gp
     if (current->mm != va_space->va_space_mm.mm)
         return NV_ERR_NOT_SUPPORTED;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(current->mm);
 
     // pnv_npu2_init_context() doesn't handle being called multiple times for
     // the same GPU under the same mm, which could happen if multiple VA spaces
@@ -468,7 +468,7 @@ NV_STATUS uvm_ats_ibm_service_fault(uvm_gpu_va_space_t *gpu_va_space,
     UVM_ASSERT(g_uvm_global.ats.enabled);
     UVM_ASSERT(gpu_va_space->ats.enabled);
     UVM_ASSERT(mm);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     // TODO: Bug 2103669: Service more than a single fault at a time
     ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)fault_addr, 1, write, force, &page, NULL);
diff --git a/nvidia-uvm/uvm8_gpu_access_counters.c b/nvidia-uvm/uvm8_gpu_access_counters.c
index 8a57c23..9655b23 100644
--- a/nvidia-uvm/uvm8_gpu_access_counters.c
+++ b/nvidia-uvm/uvm8_gpu_access_counters.c
@@ -1174,7 +1174,7 @@ static NV_STATUS service_phys_single_va_block(uvm_gpu_t *gpu,
         // in order to lock it before locking the VA space.
         mm = uvm_va_space_mm_retain(va_space);
         if (mm)
-            uvm_down_read_mmap_sem(&mm->mmap_sem);
+            uvm_down_read_mmap_lock(mm);
 
         // Re-check that the VA block is valid after taking the VA space lock
         uvm_va_space_down_read(va_space);
@@ -1216,7 +1216,7 @@ done:
         uvm_va_space_up_read(va_space);
 
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(mm);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
index 0d3d305..f214a76 100644
--- a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
@@ -508,7 +508,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
     // can only service managed faults, not ATS/HMM faults.
     mm = uvm_va_space_mm_retain(va_space);
     if (mm)
-        uvm_down_read_mmap_sem(&mm->mmap_sem);
+        uvm_down_read_mmap_lock(mm);
 
     uvm_va_space_down_read(va_space);
 
@@ -559,7 +559,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
 exit_no_channel:
     uvm_va_space_up_read(va_space);
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(mm);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_replayable_faults.c b/nvidia-uvm/uvm8_gpu_replayable_faults.c
index d92e693..3fa4c88 100644
--- a/nvidia-uvm/uvm8_gpu_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_replayable_faults.c
@@ -1463,7 +1463,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
 
                 uvm_va_space_up_read(va_space);
                 if (mm) {
-                    uvm_up_read_mmap_sem(&mm->mmap_sem);
+                    uvm_up_read_mmap_lock(mm);
                     uvm_va_space_mm_release(va_space);
                     mm = NULL;
                 }
@@ -1479,7 +1479,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
             // can only service managed faults, not ATS/HMM faults.
             mm = uvm_va_space_mm_retain(va_space);
             if (mm)
-                uvm_down_read_mmap_sem(&mm->mmap_sem);
+                uvm_down_read_mmap_lock(mm);
 
             uvm_va_space_down_read(va_space);
 
@@ -1586,7 +1586,7 @@ fail:
     if (va_space != NULL) {
         uvm_va_space_up_read(va_space);
         if (mm) {
-            uvm_up_read_mmap_sem(&mm->mmap_sem);
+            uvm_up_read_mmap_lock(mm);
             uvm_va_space_mm_release(va_space);
         }
     }
diff --git a/nvidia-uvm/uvm8_hmm.c b/nvidia-uvm/uvm8_hmm.c
index 67e81b6..da2cea8 100644
--- a/nvidia-uvm/uvm8_hmm.c
+++ b/nvidia-uvm/uvm8_hmm.c
@@ -126,7 +126,7 @@ NV_STATUS uvm_hmm_mirror_register(uvm_va_space_t *va_space)
     if (!uvm_hmm_is_enabled(va_space))
         return NV_OK;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(current->mm);
     uvm_assert_rwsem_locked_write(&va_space->lock);
 
     va_space->hmm_va_space.mirror.ops = &mirror_ops;
diff --git a/nvidia-uvm/uvm8_lock.c b/nvidia-uvm/uvm8_lock.c
index bf53eda..c10cd27 100644
--- a/nvidia-uvm/uvm8_lock.c
+++ b/nvidia-uvm/uvm8_lock.c
@@ -34,7 +34,7 @@ const char *uvm_lock_order_to_string(uvm_lock_order_t lock_order)
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_GLOBAL_PM);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_GLOBAL);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_ISR);
-        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_MMAP_SEM);
+        UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_MMAP_LOCK);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACES_LIST);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS);
         UVM_ENUM_STRING_CASE(UVM_LOCK_ORDER_VA_SPACE_READ_ACQUIRE_WRITE_RELEASE_LOCK);
@@ -88,8 +88,8 @@ bool __uvm_record_lock(void *lock, uvm_lock_order_t lock_order, uvm_lock_flags_t
     //       any new invalid uses while we figure out a better way to handle
     //       these dependencies.
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS) {
-        if (test_bit(UVM_LOCK_ORDER_MMAP_SEM, uvm_context->acquired_lock_orders)) {
-            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_sem held\n");
+        if (test_bit(UVM_LOCK_ORDER_MMAP_LOCK, uvm_context->acquired_lock_orders)) {
+            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_lock held\n");
             correct = false;
         }
 
diff --git a/nvidia-uvm/uvm8_lock.h b/nvidia-uvm/uvm8_lock.h
index 32cfd47..d73f22c 100644
--- a/nvidia-uvm/uvm8_lock.h
+++ b/nvidia-uvm/uvm8_lock.h
@@ -44,7 +44,7 @@
 //      sleep cycles.
 //
 //      This lock is special: while it's taken by user-facing entry points,
-//      and may be taken before or after mmap_sem, this apparent violation of
+//      and may be taken before or after mmap_lock, this apparent violation of
 //      lock ordering is permissible because pm_lock may only be taken via
 //      trylock in read mode by paths which already hold any lower-level
 //      locks, as well as by paths subject to the kernel's freezer.  Paths
@@ -54,7 +54,7 @@
 //      infrequently, and only as part of to power management.  Starvation is
 //      not a concern.
 //
-//      The mmap_sem deadlock potential aside, the trylock approch is also
+//      The mmap_lock deadlock potential aside, the trylock approch is also
 //      motivated by the need to prevent user threads making UVM system calls
 //      from blocking when UVM is suspended: when the kernel suspends the
 //      system, the freezer employed to stop user tasks requires these tasks
@@ -217,8 +217,8 @@
 //      Order: UVM_LOCK_ORDER_VA_SPACE
 //      Reader/writer lock (rw_semaphore) per uvm_va_space (UVM struct file)
 //
-//      This is the UVM equivalent of mmap_sem. It protects all state under that
-//      va_space, such as the VA range tree.
+//      This is the UVM equivalent of mmap_lock. It protects all state under 
+//      that va_space, such as the VA range tree.
 //
 //      Read mode: Faults (CPU and GPU), mapping creation, prefetches. These
 //      will be serialized at the VA block level if necessary. RM calls are
@@ -356,7 +356,7 @@ typedef enum
     UVM_LOCK_ORDER_GLOBAL_PM,
     UVM_LOCK_ORDER_GLOBAL,
     UVM_LOCK_ORDER_ISR,
-    UVM_LOCK_ORDER_MMAP_SEM,
+    UVM_LOCK_ORDER_MMAP_LOCK,
     UVM_LOCK_ORDER_VA_SPACES_LIST,
     UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS,
     UVM_LOCK_ORDER_VA_SPACE_READ_ACQUIRE_WRITE_RELEASE_LOCK,
@@ -450,29 +450,30 @@ bool __uvm_locking_initialized(void);
   // the given mode.
   #define uvm_check_locked(lock, flags) __uvm_check_locked((lock), (lock)->lock_order, (flags))
 
-  // Helpers for recording and asserting mmap_sem state
-  #define uvm_record_lock_mmap_sem_read(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
+  // Helpers for recording and asserting mmap_lock
+  // (mmap_sem in kernels < 5.8 ) state
+  #define uvm_record_lock_mmap_lock_read(mm) \
+          uvm_record_lock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
+  #define uvm_record_unlock_mmap_lock_read(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, \
+  #define uvm_record_unlock_mmap_lock_read_out_of_order(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, \
                                 UVM_LOCK_FLAGS_MODE_SHARED | UVM_LOCK_FLAGS_OUT_OF_ORDER)
 
-  #define uvm_record_lock_mmap_sem_write(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+  #define uvm_record_lock_mmap_lock_write(mm) \
+          uvm_record_lock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+  #define uvm_record_unlock_mmap_lock_write(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, \
+  #define uvm_record_unlock_mmap_lock_write_out_of_order(mm) \
+          uvm_record_unlock_raw(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, \
                                 UVM_LOCK_FLAGS_MODE_EXCLUSIVE | UVM_LOCK_FLAGS_OUT_OF_ORDER)
 
-  #define uvm_check_locked_mmap_sem(mmap_sem, flags) \
-           __uvm_check_locked((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, (flags))
+  #define uvm_check_locked_mmap_lock(mm, flags) \
+           __uvm_check_locked(nv_mmap_get_lock(mm), UVM_LOCK_ORDER_MMAP_LOCK, (flags))
 
   // Helpers for recording RM API lock usage around UVM-RM interfaces
   #define uvm_record_lock_rm_api() \
@@ -505,14 +506,14 @@ bool __uvm_locking_initialized(void);
       return false;
   }
 
-  #define uvm_record_lock_mmap_sem_read                 UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read               UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read_out_of_order  UVM_IGNORE_EXPR
-  #define uvm_record_lock_mmap_sem_write                UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write              UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write_out_of_order UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_read                 UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read               UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read_out_of_order  UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_write                UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write              UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write_out_of_order UVM_IGNORE_EXPR
 
-  #define uvm_check_locked_mmap_sem                     uvm_check_locked
+  #define uvm_check_locked_mmap_lock                     uvm_check_locked
 
   #define uvm_record_lock_rm_api()
   #define uvm_record_unlock_rm_api()
@@ -529,47 +530,48 @@ bool __uvm_locking_initialized(void);
 #define uvm_assert_lockable_order(order) UVM_ASSERT(__uvm_check_lockable_order(order, UVM_LOCK_FLAGS_MODE_ANY))
 #define uvm_assert_unlocked_order(order) UVM_ASSERT(__uvm_check_unlocked_order(order))
 
-// Helpers for locking mmap_sem and recording its usage
-#define uvm_assert_mmap_sem_locked_mode(mmap_sem, flags) ({                          \
-      typeof(mmap_sem) _sem = (mmap_sem);                                            \
-      UVM_ASSERT(rwsem_is_locked(_sem) && uvm_check_locked_mmap_sem(_sem, (flags))); \
+// Helpers for locking mmap_lock (mmap_sem in kernels < 5.8)
+// and recording its usage
+#define uvm_assert_mmap_lock_locked_mode(mm, flags) ({                                      \
+      typeof(mm) _mm = (mm);                                                                \
+      UVM_ASSERT(nv_mm_rwsem_is_locked(_mm) && uvm_check_locked_mmap_lock((_mm), (flags))); \
   })
 
-#define uvm_assert_mmap_sem_locked(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_ANY)
-#define uvm_assert_mmap_sem_locked_read(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_SHARED)
-#define uvm_assert_mmap_sem_locked_write(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
-
-#define uvm_down_read_mmap_sem(mmap_sem) ({             \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_read(_sem);            \
-        down_read(_sem);                                \
+#define uvm_assert_mmap_lock_locked(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_FLAGS_MODE_ANY)
+#define uvm_assert_mmap_lock_locked_read(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_FLAGS_MODE_SHARED)
+#define uvm_assert_mmap_lock_locked_write(mm) \
+        uvm_assert_mmap_lock_locked_mode((mm), UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+
+#define uvm_down_read_mmap_lock(mm) ({                  \
+        typeof(mm) _mm = (mm);                          \
+        uvm_record_lock_mmap_lock_read(_mm);            \
+        nv_mmap_read_lock(_mm);                         \
     })
 
-#define uvm_up_read_mmap_sem(mmap_sem) ({               \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_read(_sem);                                  \
-        uvm_record_unlock_mmap_sem_read(_sem);          \
+#define uvm_up_read_mmap_lock(mm) ({                    \
+        typeof(mm) _mm = (mm);                          \
+        nv_mmap_read_unlock(_mm);                       \
+        uvm_record_unlock_mmap_lock_read(_mm);          \
     })
 
-#define uvm_up_read_mmap_sem_out_of_order(mmap_sem) ({      \
-        typeof(mmap_sem) _sem = (mmap_sem);                 \
-        up_read(_sem);                                      \
-        uvm_record_unlock_mmap_sem_read_out_of_order(_sem); \
+#define uvm_up_read_mmap_lock_out_of_order(mm) ({           \
+        typeof(mm) _mm = (mm);                              \
+        nv_mmap_read_unlock(_mm);                           \
+        uvm_record_unlock_mmap_lock_read_out_of_order(_mm); \
     })
 
-#define uvm_down_write_mmap_sem(mmap_sem) ({            \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_write(_sem);           \
-        down_write(_sem);                               \
+#define uvm_down_write_mmap_lock(mm) ({                 \
+        typeof(mm) _mm = (mm);                          \
+        uvm_record_lock_mmap_lock_write(_mm);           \
+        nv_mmap_write_lock(_mm);                        \
     })
 
-#define uvm_up_write_mmap_sem(mmap_sem) ({              \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_write(_sem);                                 \
-        uvm_record_unlock_mmap_sem_write(_sem);         \
+#define uvm_up_write_mmap_lock(mm) ({                   \
+        typeof(mm) _mm = (mm);                          \
+        nv_mmap_write_unlock(_mm);                      \
+        uvm_record_unlock_mmap_lock_write(_mm);         \
     })
 
 // Helper for calling a UVM-RM interface function with lock recording
@@ -740,6 +742,20 @@ typedef struct
 
 #define uvm_assert_mutex_unlocked(uvm_mutex) UVM_ASSERT(!mutex_is_locked(&(uvm_mutex)->m))
 
+//
+// Linux kernel mutexes cannot be used with interrupts disabled. Doing so
+// can lead to deadlocks.
+// To warn about mutex usages with interrupts disabled, the following
+// macros and inline functions wrap around the raw kernel mutex operations
+// in order to check if the interrupts have been disabled and assert if so.
+//
+// TODO: Bug 2690258: evaluate whether !irqs_disabled() && !in_interrupt() is
+//       enough.
+//
+#define uvm_assert_mutex_interrupts() ({                                                                        \
+        UVM_ASSERT_MSG(!irqs_disabled() && !in_interrupt(), "Mutexes cannot be used with interrupts disabled"); \
+    })
+
 //
 // Linux kernel mutexes cannot be used with interrupts disabled. Doing so
 // can lead to deadlocks.
@@ -820,6 +836,8 @@ static void uvm_sema_init(uvm_semaphore_t *semaphore, int val, uvm_lock_order_t
 
 #define uvm_sem_is_locked(uvm_sem) uvm_check_locked(uvm_sem, UVM_LOCK_FLAGS_MODE_SHARED)
 
+#define uvm_sem_is_locked(uvm_sem) uvm_check_locked(uvm_sem, UVM_LOCK_FLAGS_MODE_SHARED)
+
 #define uvm_down(uvm_sem) ({                               \
         typeof(uvm_sem) _sem = (uvm_sem);                  \
         uvm_record_lock(_sem, UVM_LOCK_FLAGS_MODE_SHARED); \
diff --git a/nvidia-uvm/uvm8_lock_test.c b/nvidia-uvm/uvm8_lock_test.c
index 583f0db..926b593 100644
--- a/nvidia-uvm/uvm8_lock_test.c
+++ b/nvidia-uvm/uvm8_lock_test.c
@@ -64,7 +64,7 @@ static bool fake_check_locked(uvm_lock_order_t lock_order, uvm_lock_flags_t flag
 
 // TODO: Bug 1799173: The lock asserts verify that the RM GPU lock isn't taken
 //       with the VA space lock in exclusive mode, and that the RM GPU lock
-//       isn't taken with mmap_sem held in any mode. Hack around this in the
+//       isn't taken with mmap_lock held in any mode. Hack around this in the
 //       test to enable the checks until we figure out something better.
 static bool skip_lock(uvm_lock_order_t lock_order, uvm_lock_flags_t flags)
 {
@@ -73,7 +73,7 @@ static bool skip_lock(uvm_lock_order_t lock_order, uvm_lock_flags_t flags)
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS)
         return mode_flags == UVM_LOCK_FLAGS_MODE_EXCLUSIVE;
 
-    return lock_order == UVM_LOCK_ORDER_MMAP_SEM;
+    return lock_order == UVM_LOCK_ORDER_MMAP_LOCK;
 }
 
 static NV_STATUS test_all_locks_from(uvm_lock_order_t from_lock_order)
diff --git a/nvidia-uvm/uvm8_mem.c b/nvidia-uvm/uvm8_mem.c
index f88de44..dcb9d1b 100644
--- a/nvidia-uvm/uvm8_mem.c
+++ b/nvidia-uvm/uvm8_mem.c
@@ -482,7 +482,7 @@ static NV_STATUS uvm_mem_map_cpu_to_sysmem_user(uvm_mem_t *mem, struct vm_area_s
 
     UVM_ASSERT(uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
 
     // TODO: Bug 1995015: high-order page allocations need to be allocated as
     // compound pages in order to be able to use vm_insert_page on them. This
@@ -506,7 +506,7 @@ static NV_STATUS uvm_mem_map_cpu_to_vidmem_user(uvm_mem_t *mem, struct vm_area_s
     size_t num_chunk_pages = mem->chunk_size / PAGE_SIZE;
 
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     UVM_ASSERT(!uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->backing_gpu != NULL);
     UVM_ASSERT(mem->backing_gpu->parent->numa_info.enabled);
diff --git a/nvidia-uvm/uvm8_migrate.c b/nvidia-uvm/uvm8_migrate.c
index 90a0fc8..da199e8 100644
--- a/nvidia-uvm/uvm8_migrate.c
+++ b/nvidia-uvm/uvm8_migrate.c
@@ -596,7 +596,7 @@ static NV_STATUS uvm_migrate(uvm_va_space_t *va_space,
     bool is_single_block;
     bool should_do_cpu_preunmap;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(current->mm);
     uvm_assert_rwsem_locked(&va_space->lock);
 
     if (!first_va_range || first_va_range->type != UVM_VA_RANGE_TYPE_MANAGED)
@@ -855,7 +855,7 @@ NV_STATUS uvm_api_migrate(UVM_MIGRATE_PARAMS *params, struct file *filp)
     }
 
     // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (!is_async) {
@@ -937,7 +937,7 @@ done:
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(current->mm);
 
     if (tracker_ptr) {
         if (params->semaphoreAddress && status == NV_OK) {
@@ -983,7 +983,7 @@ NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, st
     uvm_gpu_t *gpu = NULL;
 
     // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->destinationUuid)) {
@@ -1028,7 +1028,7 @@ done:
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(current->mm);
 
     tracker_status = uvm_tracker_wait_deinit(&local_tracker);
     uvm_va_space_up_read(va_space);
diff --git a/nvidia-uvm/uvm8_migrate_pageable.c b/nvidia-uvm/uvm8_migrate_pageable.c
index dbb7b03..82c8e07 100644
--- a/nvidia-uvm/uvm8_migrate_pageable.c
+++ b/nvidia-uvm/uvm8_migrate_pageable.c
@@ -756,7 +756,7 @@ static NV_STATUS migrate_pageable_vma_region(struct vm_area_struct *vma,
     UVM_ASSERT(start >= vma->vm_start);
     UVM_ASSERT(outer <= vma->vm_end);
     UVM_ASSERT(outer - start <= UVM_MIGRATE_VMA_MAX_SIZE);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
     uvm_assert_rwsem_locked(&migrate_vma_state->va_space->lock);
 
     ret = migrate_vma(&g_migrate_vma_ops,
@@ -830,7 +830,7 @@ static NV_STATUS migrate_pageable_vma(struct vm_area_struct *vma,
     UVM_ASSERT(PAGE_ALIGNED(outer));
     UVM_ASSERT(vma->vm_end > start);
     UVM_ASSERT(vma->vm_start < outer);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
     uvm_assert_rwsem_locked(&va_space->lock);
 
     // Adjust to input range boundaries
@@ -882,7 +882,7 @@ static NV_STATUS migrate_pageable(struct mm_struct *mm,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     vma = find_vma_intersection(mm, start, outer);
 
@@ -944,7 +944,7 @@ NV_STATUS uvm_migrate_pageable(uvm_va_space_t *va_space,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     // We only check that dst_cpu_node_id is a valid node in the system and it
     // doesn't correspond to a GPU node. This is fine because alloc_pages_node
diff --git a/nvidia-uvm/uvm8_policy.c b/nvidia-uvm/uvm8_policy.c
index a912963..1bf6de2 100644
--- a/nvidia-uvm/uvm8_policy.c
+++ b/nvidia-uvm/uvm8_policy.c
@@ -38,7 +38,7 @@ bool uvm_is_valid_vma_range(NvU64 start, NvU64 length)
     const NvU64 end = start + length;
     struct vm_area_struct *vma;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(current->mm);
 
     vma = find_vma_intersection(current->mm, start, end);
 
@@ -233,7 +233,7 @@ NV_STATUS uvm_api_set_preferred_location(const UVM_SET_PREFERRED_LOCATION_PARAMS
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
     has_va_space_write_lock = true;
 
@@ -306,7 +306,7 @@ done:
         uvm_va_space_up_write(va_space);
     else
         uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status == NV_OK ? tracker_status : status;
 }
@@ -318,7 +318,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, params->requestedBase, params->length);
@@ -329,7 +329,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
         status = NV_OK;
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
@@ -410,7 +410,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -474,7 +474,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status;
 }
@@ -657,7 +657,7 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
     UVM_ASSERT(va_space);
 
     // We need mmap_sem as we may create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -711,7 +711,8 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
+
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_populate_pageable.c b/nvidia-uvm/uvm8_populate_pageable.c
index c47975e..10e5a71 100644
--- a/nvidia-uvm/uvm8_populate_pageable.c
+++ b/nvidia-uvm/uvm8_populate_pageable.c
@@ -48,7 +48,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     UVM_ASSERT(PAGE_ALIGNED(outer));
     UVM_ASSERT(vma->vm_end > start);
     UVM_ASSERT(vma->vm_start < outer);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     if (!min_prot_ok)
         return NV_ERR_INVALID_ADDRESS;
@@ -66,12 +66,12 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     // page fault handler. The page fault is caused by get_user_pages.
     uvm_managed_vma = uvm_file_is_nvidia_uvm(vma->vm_file);
     if (uvm_managed_vma)
-        uvm_record_unlock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_read(mm);
 
     ret = NV_GET_USER_PAGES(start, vma_num_pages, is_writable, 0, NULL, NULL);
 
     if (uvm_managed_vma)
-        uvm_record_lock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_lock_mmap_lock_read(mm);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
@@ -94,7 +94,7 @@ NV_STATUS uvm_populate_pageable(struct mm_struct *mm,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(mm);
 
     vma = find_vma_intersection(mm, start, outer);
 
@@ -155,14 +155,14 @@ NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params,
 
     // mmap_sem is needed to traverse the vmas in the input range and call into
     // get_user_pages
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
 
     if (allow_managed || uvm_va_space_range_empty(va_space, params->base, params->base + params->length - 1))
         status = uvm_populate_pageable(current->mm, params->base, params->length, min_prot);
     else
         status = NV_ERR_INVALID_ADDRESS;
 
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     return status;
 }
diff --git a/nvidia-uvm/uvm8_tools.c b/nvidia-uvm/uvm8_tools.c
index 0dae47e..620e1cc 100644
--- a/nvidia-uvm/uvm8_tools.c
+++ b/nvidia-uvm/uvm8_tools.c
@@ -264,9 +264,9 @@ static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct p
         goto fail;
     }
 
-    down_read(&current->mm->mmap_sem);
+    nv_mmap_read_lock(current->mm);
     ret = NV_GET_USER_PAGES(user_va, num_pages, 1, 0, *pages, vmas);
-    up_read(&current->mm->mmap_sem);
+    nv_mmap_read_unlock(current->mm);
     if (ret != num_pages) {
         status = NV_ERR_INVALID_ARGUMENT;
         goto fail;
diff --git a/nvidia-uvm/uvm8_va_block.c b/nvidia-uvm/uvm8_va_block.c
index 97e8780..a8dabf1 100644
--- a/nvidia-uvm/uvm8_va_block.c
+++ b/nvidia-uvm/uvm8_va_block.c
@@ -6528,7 +6528,7 @@ static NV_STATUS block_map_cpu_page_to(uvm_va_block_t *block,
     // us, so we can safely operate on the vma but we can't use
     // uvm_va_range_vma_current.
     vma = uvm_va_range_vma(va_range);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     UVM_ASSERT(!uvm_va_space_mm_enabled(va_space) || va_space->va_space_mm.mm == vma->vm_mm);
 
     // Add the mapping
@@ -10675,7 +10675,7 @@ NV_STATUS uvm8_test_change_pte_mapping(UVM_TEST_CHANGE_PTE_MAPPING_PARAMS *param
 
     // mmap_sem isn't needed for invalidating CPU mappings, but it will be
     // needed for inserting them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->uuid)) {
@@ -10758,7 +10758,7 @@ out_block:
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_va_block_context_free(block_context);
 
@@ -10773,7 +10773,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
     BUILD_BUG_ON(UVM_TEST_VA_BLOCK_SIZE != UVM_VA_BLOCK_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     status = uvm_va_block_find(va_space, params->lookup_address, &va_block);
@@ -10791,7 +10791,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
@@ -10808,7 +10808,7 @@ NV_STATUS uvm8_test_va_residency_info(UVM_TEST_VA_RESIDENCY_INFO_PARAMS *params,
     unsigned release_block_count = 0;
     NvU64 addr = UVM_ALIGN_DOWN(params->lookup_address, PAGE_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, addr);
@@ -10951,7 +10951,7 @@ out:
             uvm_va_block_release(block);
     }
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.c b/nvidia-uvm/uvm8_va_range.c
index c390027..d4e37d8 100644
--- a/nvidia-uvm/uvm8_va_range.c
+++ b/nvidia-uvm/uvm8_va_range.c
@@ -1804,7 +1804,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
     va_space = uvm_va_space_get(filp);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, params->lookup_address);
@@ -1865,7 +1865,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.h b/nvidia-uvm/uvm8_va_range.h
index 81b17ec..9928f60 100644
--- a/nvidia-uvm/uvm8_va_range.h
+++ b/nvidia-uvm/uvm8_va_range.h
@@ -712,7 +712,7 @@ static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, s
     if (mm != vma->vm_mm)
         return NULL;
 
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(vma->vm_mm);
     return vma;
 }
 
diff --git a/nvidia-uvm/uvm8_va_space.c b/nvidia-uvm/uvm8_va_space.c
index 1c1f116..2ec3ad6 100644
--- a/nvidia-uvm/uvm8_va_space.c
+++ b/nvidia-uvm/uvm8_va_space.c
@@ -516,7 +516,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
     if (flags & ~UVM_INIT_FLAGS_MASK)
         return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_write_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     if (atomic_read(&va_space->initialized)) {
@@ -545,7 +545,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_write_mmap_lock(current->mm);
 
     return status;
 }
@@ -835,7 +835,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
 
     // The mmap_sem lock is needed to establish CPU mappings to any pages
     // evicted from the GPU if accessed by CPU is set for them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -852,7 +852,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
     uvm_processor_mask_clear(&va_space->gpu_unregister_in_progress, gpu->id);
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_deferred_free_object_list(&deferred_free_list);
 
@@ -1285,9 +1285,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     // need mmap_sem in read mode to handle potential CPU mapping changes in
     // uvm_va_range_add_gpu_va_space().
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_write_mmap_lock(current->mm);
     else
-        uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_read_mmap_lock(current->mm);
 
     uvm_va_space_down_write(va_space);
 
@@ -1342,9 +1342,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(current->mm);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(current->mm);
 
     uvm_gpu_release(gpu);
     return NV_OK;
@@ -1363,9 +1363,9 @@ error:
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(current->mm);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(current->mm);
 
     destroy_gpu_va_space(gpu_va_space);
 
@@ -1439,7 +1439,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     uvm_gpu_retain(gpu);
     uvm_va_space_up_read_rm(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(current->mm);
     uvm_va_space_down_write(va_space);
 
     // We dropped the lock so we have to re-verify that this gpu_va_space is
@@ -1457,7 +1457,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     }
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(current->mm);
 
     uvm_deferred_free_object_list(&deferred_free_list);
     uvm_gpu_va_space_release(gpu_va_space);
diff --git a/nvidia-uvm/uvm8_va_space_mm.c b/nvidia-uvm/uvm8_va_space_mm.c
index 73988de..2e5e7ec 100644
--- a/nvidia-uvm/uvm8_va_space_mm.c
+++ b/nvidia-uvm/uvm8_va_space_mm.c
@@ -246,7 +246,7 @@ bool uvm_va_space_mm_enabled(uvm_va_space_t *va_space)
     static int uvm_mmu_notifier_register(uvm_va_space_mm_t *va_space_mm)
     {
         UVM_ASSERT(va_space_mm->mm);
-        uvm_assert_mmap_sem_locked_write(&va_space_mm->mm->mmap_sem);
+        uvm_assert_mmap_lock_locked_write(va_space_mm->mm);
 
         if (UVM_ATS_IBM_SUPPORTED_IN_DRIVER() && g_uvm_global.ats.enabled)
             va_space_mm->mmu_notifier.ops = &uvm_mmu_notifier_ops_ats;
@@ -278,7 +278,7 @@ NV_STATUS uvm_va_space_mm_register(uvm_va_space_t *va_space)
     uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
     int ret;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(current->mm);
     uvm_assert_rwsem_locked_write(&va_space->lock);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) != NV_OK);
@@ -313,7 +313,7 @@ void uvm_va_space_mm_unregister(uvm_va_space_t *va_space)
     // mmu_notifier_unregister() may trigger uvm_va_space_mm_shutdown(), which
     // takes those locks and also waits for other threads which may take those
     // locks.
-    uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_SEM);
+    uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_LOCK);
     uvm_assert_unlocked_order(UVM_LOCK_ORDER_VA_SPACE);
 
     if (!va_space_mm->mm)
@@ -546,9 +546,9 @@ static NV_STATUS mm_read64(struct mm_struct *mm, NvU64 addr, NvU64 *val)
 
     UVM_ASSERT(IS_ALIGNED(addr, sizeof(val)));
 
-    uvm_down_read_mmap_sem(&mm->mmap_sem);
+    uvm_down_read_mmap_lock(mm);
     ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)addr, 1, write, force, &page, NULL);
-    uvm_up_read_mmap_sem(&mm->mmap_sem);
+    uvm_up_read_mmap_lock(mm);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
diff --git a/nvidia/nvidia.Kbuild b/nvidia/nvidia.Kbuild
index f0b75c6..424f6e6 100644
--- a/nvidia/nvidia.Kbuild
+++ b/nvidia/nvidia.Kbuild
@@ -176,7 +176,9 @@ NV_CONFTEST_TYPE_COMPILE_TESTS += node_states_n_memory
 NV_CONFTEST_TYPE_COMPILE_TESTS += kmem_cache_has_kobj_remove_work
 NV_CONFTEST_TYPE_COMPILE_TESTS += sysfs_slab_unlink
 NV_CONFTEST_TYPE_COMPILE_TESTS += proc_ops
+NV_CONFTEST_TYPE_COMPILE_TESTS += vmalloc_has_pgprot_t_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += timeval
+NV_CONFTEST_TYPE_COMPILE_TESTS += mm_has_mmap_lock
 
 NV_CONFTEST_GENERIC_COMPILE_TESTS += dom0_kernel_present
 NV_CONFTEST_GENERIC_COMPILE_TESTS += nvidia_vgpu_hyperv_available
diff --git a/nvidia/nvlink_linux.c b/nvidia/nvlink_linux.c
index 56f629e..168477a 100644
--- a/nvidia/nvlink_linux.c
+++ b/nvidia/nvlink_linux.c
@@ -315,7 +315,6 @@ static long nvlink_fops_unlocked_ioctl(struct file *file,
     return nvlink_fops_ioctl(NV_FILE_INODE(file), file, cmd, arg);
 }
 
-
 static const struct file_operations nvlink_fops = {
     .owner           = THIS_MODULE,
     .open            = nvlink_fops_open,
diff --git a/nvidia/os-mlock.c b/nvidia/os-mlock.c
index 1900fa1..237e2ec 100644
--- a/nvidia/os-mlock.c
+++ b/nvidia/os-mlock.c
@@ -43,7 +43,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
 
     vma = find_vma(mm, (NvUPtr)address);
     if ((vma == NULL) || ((vma->vm_flags & (VM_IO | VM_PFNMAP)) == 0))
@@ -76,7 +76,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
     }
 
 done:
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
 
     return rmStatus;
 }
@@ -110,10 +110,10 @@ NV_STATUS NV_API_CALL os_lock_user_pages(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    nv_mmap_read_lock(mm);
     ret = NV_GET_USER_PAGES((unsigned long)address,
                             page_count, write, force, user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    nv_mmap_read_unlock(mm);
     pinned = ret;
 
     if (ret < 0)
-- 
2.25.1

